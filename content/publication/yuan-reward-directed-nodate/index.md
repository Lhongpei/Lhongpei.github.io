---
title: 'Reward-Directed Conditional Diffusion: Provable Distribution Estimation and
  Reward Improvement'
authors:
- Hui Yuan
- Kaixuan Huang
date: -01-01
publishDate: '2024-11-04T04:02:22.765514Z'
publication_types:
- article-journal
abstract: 'We explore the methodology and theory of reward-directed generation via
  conditional diffusion models. Directed generation aims to generate samples with
  desired properties as measured by a reward function, which has broad applications
  in generative AI, reinforcement learning, and computational biology. We consider
  the common learning scenario where the dataset consists of majorly unlabeled data
  and a small set of data with noisy reward labels. Our approach leverages a learned
  reward function on the smaller data set as a pseudolabeler to label the unlabelled
  data. After pseudo-labelling, a conditional diffusion model (CDM) is trained on
  the data and samples are generated by setting a target value a as the condition
  in CDM. From a theoretical standpoint, we show that this directed generator can
  effectively learn and sample from the reward-conditioned data distribution: 1. our
  model is capable of recovering the dataâ€™s latent subspace representation. 2. the
  model generates samples moving closer to the user-specified target. The improvement
  in rewards of samples is influenced by a interplay between the strength of the reward
  signal, the distribution shift, and the cost of off-support extrapolation. We provide
  empirical results to validate our theory and highlight the relationship between
  the strength of extrapolation and the quality of generated samples. Our code is
  available at https://github.com/Kaffaljidhmah2/RCGDM.'
---
