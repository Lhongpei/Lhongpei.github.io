@article{yuan_reward-directed_nodate,
 abstract = {We explore the methodology and theory of reward-directed generation via conditional diffusion models. Directed generation aims to generate samples with desired properties as measured by a reward function, which has broad applications in generative AI, reinforcement learning, and computational biology. We consider the common learning scenario where the dataset consists of majorly unlabeled data and a small set of data with noisy reward labels. Our approach leverages a learned reward function on the smaller data set as a pseudolabeler to label the unlabelled data. After pseudo-labelling, a conditional diffusion model (CDM) is trained on the data and samples are generated by setting a target value a as the condition in CDM. From a theoretical standpoint, we show that this directed generator can effectively learn and sample from the reward-conditioned data distribution: 1. our model is capable of recovering the data’s latent subspace representation. 2. the model generates samples moving closer to the user-specified target. The improvement in rewards of samples is influenced by a interplay between the strength of the reward signal, the distribution shift, and the cost of off-support extrapolation. We provide empirical results to validate our theory and highlight the relationship between the strength of extrapolation and the quality of generated samples. Our code is available at https://github.com/Kaffaljidhmah2/RCGDM.},
 author = {Yuan, Hui and Huang, Kaixuan},
 file = {Yuan 和 Huang - Reward-Directed Conditional Diffusion Provable Di.pdf:C\:\\Users\\14532\\Zotero\\storage\ŁXY9ZZRN\\Yuan 和 Huang - Reward-Directed Conditional Diffusion Provable Di.pdf:application/pdf},
 language = {en},
 title = {Reward-Directed Conditional Diffusion: Provable Distribution Estimation and Reward Improvement}
}
