
@misc{echeverria_solving_2024,
	title = {Solving the flexible job-shop scheduling problem through an enhanced deep reinforcement learning approach},
	url = {http://arxiv.org/abs/2310.15706},
	abstract = {In scheduling problems common in the industry and various real-world scenarios, responding in real-time to disruptive events is essential. Recent methods propose the use of deep reinforcement learning (DRL) to learn policies capable of generating solutions under this constraint. The objective of this paper is to introduce a new DRL method for solving the flexible job-shop scheduling problem, particularly for large instances. The approach is based on the use of heterogeneous graph neural networks to a more informative graph representation of the problem. This novel modeling of the problem enhances the policy's ability to capture state information and improve its decision-making capacity. Additionally, we introduce two novel approaches to enhance the performance of the DRL approach: the first involves generating a diverse set of scheduling policies, while the second combines DRL with dispatching rules (DRs) constraining the action space. Experimental results on two public benchmarks show that our approach outperforms DRs and achieves superior results compared to three state-of-the-art DRL methods, particularly for large instances.},
	urldate = {2024-03-22},
	publisher = {arXiv},
	author = {Echeverria, Imanol and Murua, Maialen and Santana, Roberto},
	month = jan,
	year = {2024},
	note = {arXiv:2310.15706 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\14532\\Zotero\\storage\\47UDYAUI\\2310.html:text/html;Full Text PDF:C\:\\Users\\14532\\Zotero\\storage\\P3ISZS8D\\Echeverria 等 - 2024 - Solving the flexible job-shop scheduling problem t.pdf:application/pdf},
}

@misc{wang_flexible_2023,
	title = {Flexible {Job} {Shop} {Scheduling} via {Dual} {Attention} {Network} {Based} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2305.05119},
	abstract = {Flexible manufacturing has given rise to complex scheduling problems such as the flexible job shop scheduling problem (FJSP). In FJSP, operations can be processed on multiple machines, leading to intricate relationships between operations and machines. Recent works have employed deep reinforcement learning (DRL) to learn priority dispatching rules (PDRs) for solving FJSP. However, the quality of solutions still has room for improvement relative to that by the exact methods such as OR-Tools. To address this issue, this paper presents a novel end-to-end learning framework that weds the merits of self-attention models for deep feature extraction and DRL for scalable decision-making. The complex relationships between operations and machines are represented precisely and concisely, for which a dual-attention network (DAN) comprising several interconnected operation message attention blocks and machine message attention blocks is proposed. The DAN exploits the complicated relationships to construct production-adaptive operation and machine features to support high-quality decisionmaking. Experimental results using synthetic data as well as public benchmarks corroborate that the proposed approach outperforms both traditional PDRs and the state-of-the-art DRL method. Moreover, it achieves results comparable to exact methods in certain cases and demonstrates favorable generalization ability to large-scale and real-world unseen FJSP tasks.},
	urldate = {2024-03-22},
	publisher = {arXiv},
	author = {Wang, Runqing and Wang, Gang and Sun, Jian and Deng, Fang and Chen, Jie},
	month = jun,
	year = {2023},
	note = {arXiv:2305.05119 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\14532\\Zotero\\storage\\DGUHCARU\\2305.html:text/html;Full Text PDF:C\:\\Users\\14532\\Zotero\\storage\\XGRTFFXV\\Wang 等 - 2023 - Flexible Job Shop Scheduling via Dual Attention Ne.pdf:application/pdf},
}

@misc{wong_generating_2023,
	title = {Generating {Dispatching} {Rules} for the {Interrupting} {Swap}-{Allowed} {Blocking} {Job} {Shop} {Problem} {Using} {Graph} {Neural} {Network} and {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2302.02506},
	doi = {10.48550/arXiv.2302.02506},
	abstract = {The interrupting swap-allowed blocking job shop problem (ISBJSSP) is a complex scheduling problem that is able to model many manufacturing planning and logistics applications realistically by addressing both the lack of storage capacity and unforeseen production interruptions. Subjected to random disruptions due to machine malfunction or maintenance, industry production settings often choose to adopt dispatching rules to enable adaptive, real-time re-scheduling, rather than traditional methods that require costly re-computation on the new configuration every time the problem condition changes dynamically. To generate dispatching rules for the ISBJSSP problem, we introduce a dynamic disjunctive graph formulation characterized by nodes and edges subjected to continuous deletions and additions. This formulation enables the training of an adaptive scheduler utilizing graph neural networks and reinforcement learning. Furthermore, a simulator is developed to simulate interruption, swapping, and blocking in the ISBJSSP setting. Employing a set of reported benchmark instances, we conduct a detailed experimental study on ISBJSSP instances with a range of machine shutdown probabilities to show that the scheduling policies generated can outperform or are at least as competitive as existing dispatching rules with predetermined priority. This study shows that the ISBJSSP, which requires real-time adaptive solutions, can be scheduled efficiently with the proposed method when production interruptions occur with random machine shutdowns.},
	urldate = {2024-03-22},
	publisher = {arXiv},
	author = {Wong, Vivian W. H. and Kim, Sang Hun and Park, Junyoung and Park, Jinkyoo and Law, Kincho H.},
	month = sep,
	year = {2023},
	note = {arXiv:2302.02506 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\14532\\Zotero\\storage\\S6UD35PE\\Wong 等 - 2023 - Generating Dispatching Rules for the Interrupting .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\14532\\Zotero\\storage\\JD8YZUQJ\\2302.html:text/html},
}

@misc{li_learning_2023,
	title = {Learning to {Optimize} {Permutation} {Flow} {Shop} {Scheduling} via {Graph}-based {Imitation} {Learning}},
	url = {http://arxiv.org/abs/2210.17178},
	doi = {10.48550/arXiv.2210.17178},
	abstract = {The permutation flow shop scheduling (PFSS), aiming at finding the optimal permutation of jobs, is widely used in manufacturing systems. When solving large-scale PFSS problems, traditional optimization algorithms such as heuristics could hardly meet the demands of both solution accuracy and computational efficiency, thus learning-based methods have recently garnered more attention. Some work attempts to solve the problems by reinforcement learning methods, which suffer from slow convergence issues during training and are still not accurate enough regarding the solutions. To that end, we propose to train the model via expert-driven imitation learning, which accelerates convergence more stably and accurately. Moreover, in order to extract better feature representations of input jobs, we incorporate the graph structure as the encoder. The extensive experiments reveal that our proposed model obtains significant promotion and presents excellent generalizability in large-scale problems with up to 1000 jobs. Compared to the state-of-the-art reinforcement learning method, our model's network parameters are reduced to only 37{\textbackslash}\% of theirs, and the solution gap of our model towards the expert solutions decreases from 6.8{\textbackslash}\% to 1.3{\textbackslash}\% on average. The code is available at: {\textbackslash}url\{https://github.com/longkangli/PFSS-IL\}.},
	urldate = {2024-03-22},
	publisher = {arXiv},
	author = {Li, Longkang and Liang, Siyuan and Zhu, Zihao and Ding, Chris and Zha, Hongyuan and Wu, Baoyuan},
	month = dec,
	year = {2023},
	note = {arXiv:2210.17178 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:C\:\\Users\\14532\\Zotero\\storage\\F9ZX8M4U\\Li 等 - 2023 - Learning to Optimize Permutation Flow Shop Schedul.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\14532\\Zotero\\storage\\Y4AMCC73\\2210.html:text/html},
}

@article{park_learning_2021,
	title = {Learning to schedule job-shop problems: {Representation} and policy learning using graph neural network and reinforcement learning},
	volume = {59},
	issn = {0020-7543, 1366-588X},
	shorttitle = {Learning to schedule job-shop problems},
	url = {http://arxiv.org/abs/2106.01086},
	doi = {10.1080/00207543.2020.1870013},
	abstract = {We propose a framework to learn to schedule a job-shop problem (JSSP) using a graph neural network (GNN) and reinforcement learning (RL). We formulate the scheduling process of JSSP as a sequential decision-making problem with graph representation of the state to consider the structure of JSSP. In solving the formulated problem, the proposed framework employs a GNN to learn that node features that embed the spatial structure of the JSSP represented as a graph (representation learning) and derive the optimum scheduling policy that maps the embedded node features to the best scheduling action (policy learning). We employ Proximal Policy Optimization (PPO) based RL strategy to train these two modules in an end-to-end fashion. We empirically demonstrate that the GNN scheduler, due to its superb generalization capability, outperforms practically favored dispatching rules and RL-based schedulers on various benchmark JSSP. We also confirmed that the proposed framework learns a transferable scheduling policy that can be employed to schedule a completely new JSSP (in terms of size and parameters) without further training.},
	number = {11},
	urldate = {2024-03-22},
	journal = {International Journal of Production Research},
	author = {Park, Junyoung and Chun, Jaehyeong and Kim, Sang Hun and Kim, Youngkook and Park, Jinkyoo},
	month = jun,
	year = {2021},
	note = {arXiv:2106.01086 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems},
	pages = {3360--3377},
	file = {arXiv Fulltext PDF:C\:\\Users\\14532\\Zotero\\storage\\HTWRX7RP\\Park 等 - 2021 - Learning to schedule job-shop problems Representa.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\14532\\Zotero\\storage\\U5F98GCS\\2106.html:text/html},
}

@article{song_flexible_2023,
	title = {Flexible {Job}-{Shop} {Scheduling} via {Graph} {Neural} {Network} and {Deep} {Reinforcement} {Learning}},
	volume = {19},
	issn = {1551-3203, 1941-0050},
	url = {https://ieeexplore.ieee.org/document/9826438/},
	doi = {10.1109/TII.2022.3189725},
	abstract = {Recently, Deep Reinforcement Learning (DRL) has been applied to learn priority dispatching rules (PDRs) for solving complex scheduling problems. However, existing works face challenges in dealing with ﬂexibility, which allows an operation to be scheduled on one out of multiple machines and is often required in practice. Such one-to-many relationship brings additional complexity in both decision-making and state representation. This paper considers the well-known Flexible Job-shop Scheduling Problem (FJSP), and addresses these issues by proposing a novel DRL method to learn high-quality PDRs end-to-end. The operation selection and machine assignment are combined as a composite decision. Moreover, based on a novel heterogeneous graph representation of scheduling states, a Heterogeneous Graph Neural Network based architecture is proposed to capture complex relationships among operations and machines. Experiments show that the proposed method outperforms traditional PDRs and is computationally efﬁcient, even on instances of larger scales and different properties unseen in training.},
	language = {en},
	number = {2},
	urldate = {2024-03-22},
	journal = {IEEE Transactions on Industrial Informatics},
	author = {Song, Wen and Chen, Xinyang and Li, Qiqiang and Cao, Zhiguang},
	month = feb,
	year = {2023},
	pages = {1600--1610},
	file = {Song 等 - 2023 - Flexible Job-Shop Scheduling via Graph Neural Netw.pdf:C\:\\Users\\14532\\Zotero\\storage\\E4SMUQLP\\Song 等 - 2023 - Flexible Job-Shop Scheduling via Graph Neural Netw.pdf:application/pdf},
}

@article{song_flexible_2023-1,
	title = {Flexible {Job}-{Shop} {Scheduling} via {Graph} {Neural} {Network} and {Deep} {Reinforcement} {Learning}},
	volume = {19},
	issn = {1551-3203, 1941-0050},
	url = {https://ieeexplore.ieee.org/document/9826438/},
	doi = {10.1109/TII.2022.3189725},
	abstract = {Recently, Deep Reinforcement Learning (DRL) has been applied to learn priority dispatching rules (PDRs) for solving complex scheduling problems. However, existing works face challenges in dealing with ﬂexibility, which allows an operation to be scheduled on one out of multiple machines and is often required in practice. Such one-to-many relationship brings additional complexity in both decision-making and state representation. This paper considers the well-known Flexible Job-shop Scheduling Problem (FJSP), and addresses these issues by proposing a novel DRL method to learn high-quality PDRs end-to-end. The operation selection and machine assignment are combined as a composite decision. Moreover, based on a novel heterogeneous graph representation of scheduling states, a Heterogeneous Graph Neural Network based architecture is proposed to capture complex relationships among operations and machines. Experiments show that the proposed method outperforms traditional PDRs and is computationally efﬁcient, even on instances of larger scales and different properties unseen in training.},
	language = {en},
	number = {2},
	urldate = {2024-03-22},
	journal = {IEEE Transactions on Industrial Informatics},
	author = {Song, Wen and Chen, Xinyang and Li, Qiqiang and Cao, Zhiguang},
	month = feb,
	year = {2023},
	pages = {1600--1610},
	file = {Song 等 - 2023 - Flexible Job-Shop Scheduling via Graph Neural Netw.pdf:C\:\\Users\\14532\\Zotero\\storage\\U5G5666Y\\Song 等 - 2023 - Flexible Job-Shop Scheduling via Graph Neural Netw.pdf:application/pdf},
}

@article{roughgarden_cs364a_nodate,
	title = {{CS364A}: {Algorithmic} {Game} {Theory} {Lecture} \#2: {Mechanism} {Design} {Basics}},
	language = {en},
	author = {Roughgarden, Tim},
	file = {Roughgarden - CS364A Algorithmic Game Theory Lecture #2 Mechan.pdf:C\:\\Users\\14532\\Zotero\\storage\\FVTJ3L9N\\Roughgarden - CS364A Algorithmic Game Theory Lecture #2 Mechan.pdf:application/pdf},
}

@article{roughgarden_cs364a_nodate-1,
	title = {{CS364A}: {Algorithmic} {Game} {Theory} {Lecture} \#3: {Myerson}’s {Lemma}},
	language = {en},
	author = {Roughgarden, Tim},
	file = {Roughgarden - CS364A Algorithmic Game Theory Lecture #3 Myerso.pdf:C\:\\Users\\14532\\Zotero\\storage\\QKBG7EUR\\Roughgarden - CS364A Algorithmic Game Theory Lecture #3 Myerso.pdf:application/pdf},
}

@article{roughgarden_cs364a_nodate-2,
	title = {{CS364A}: {Algorithmic} {Game} {Theory} {Lecture} \#4: {Algorithmic} {Mechanism} {Design}},
	language = {en},
	author = {Roughgarden, Tim},
	file = {Roughgarden - CS364A Algorithmic Game Theory Lecture #4 Algori.pdf:C\:\\Users\\14532\\Zotero\\storage\\BH6TJW66\\Roughgarden - CS364A Algorithmic Game Theory Lecture #4 Algori.pdf:application/pdf},
}

@article{roughgarden_cs364a_nodate-3,
	title = {{CS364A}: {Algorithmic} {Game} {Theory} {Lecture} \#5: {Revenue}-{Maximizing} {Auctions}},
	language = {en},
	author = {Roughgarden, Tim},
	file = {Roughgarden - CS364A Algorithmic Game Theory Lecture #5 Revenu.pdf:C\:\\Users\\14532\\Zotero\\storage\\EBP8UTTE\\Roughgarden - CS364A Algorithmic Game Theory Lecture #5 Revenu.pdf:application/pdf},
}

@article{roughgarden_cs364a_nodate-4,
	title = {{CS364A}: {Algorithmic} {Game} {Theory} {Lecture} \#6: {Simple} {Near}-{Optimal} {Auctions}},
	language = {en},
	author = {Roughgarden, Tim},
	file = {Roughgarden - CS364A Algorithmic Game Theory Lecture #6 Simple.pdf:C\:\\Users\\14532\\Zotero\\storage\\A57G9U5L\\Roughgarden - CS364A Algorithmic Game Theory Lecture #6 Simple.pdf:application/pdf},
}

@article{roughgarden_cs364a_nodate-5,
	title = {{CS364A}: {Algorithmic} {Game} {Theory} {Lecture} \#7: {Multi}-{Parameter} {Mechanism} {Design} and the {VCG} {Mechanism}},
	language = {en},
	author = {Roughgarden, Tim},
	file = {Roughgarden - CS364A Algorithmic Game Theory Lecture #7 Multi-.pdf:C\:\\Users\\14532\\Zotero\\storage\\U4HIQ6UL\\Roughgarden - CS364A Algorithmic Game Theory Lecture #7 Multi-.pdf:application/pdf},
}

@misc{noauthor_tim_nodate,
	title = {Tim {Roughgarden} 的讲稿},
	url = {https://www.timroughgarden.org/notes.html},
	urldate = {2024-03-23},
	file = {Tim Roughgarden 的讲稿:C\:\\Users\\14532\\Zotero\\storage\\7HUS38I5\\notes.html:text/html},
}

@article{roughgarden_cs364a_nodate-6,
	title = {{CS364A}: {Algorithmic} {Game} {Theory} {Lecture} \#8: {Combinatorial} and {Wireless} {Spectrum} {Auctions}},
	language = {en},
	author = {Roughgarden, Tim},
	file = {Roughgarden - CS364A Algorithmic Game Theory Lecture #8 Combin.pdf:C\:\\Users\\14532\\Zotero\\storage\\HC4ZMTEJ\\Roughgarden - CS364A Algorithmic Game Theory Lecture #8 Combin.pdf:application/pdf},
}

@article{roughgarden_cs364a_nodate-7,
	title = {{CS364A}: {Algorithmic} {Game} {Theory} {Lecture} \#9: {Beyond} {Quasi}-{Linearity}},
	language = {en},
	author = {Roughgarden, Tim},
	file = {Roughgarden - CS364A Algorithmic Game Theory Lecture #9 Beyond.pdf:C\:\\Users\\14532\\Zotero\\storage\\EVFZLLNL\\Roughgarden - CS364A Algorithmic Game Theory Lecture #9 Beyond.pdf:application/pdf},
}

@article{lei_multi-action_2022,
	title = {A multi-action deep reinforcement learning framework for flexible {Job}-shop scheduling problem},
	volume = {205},
	issn = {09574174},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417422010624},
	doi = {10.1016/j.eswa.2022.117796},
	abstract = {This paper presents an end-to-end deep reinforcement framework to automatically learn a policy for solving a flexible Job-shop scheduling problem (FJSP) using a graph neural network. In the FJSP environment, the rein­ forcement agent needs to schedule an operation belonging to a job on an eligible machine among a set of compatible machines at each timestep. This means that an agent needs to control multiple actions simulta­ neously. Such a problem with multi-actions is formulated as a multiple Markov decision process (MMDP). For solving the MMDPs, we propose a multi-pointer graph networks (MPGN) architecture and a training algorithm called multi-Proximal Policy Optimization (multi-PPO) to learn two sub-policies, including a job operation action policy and a machine action policy to assign a job operation to a machine. The MPGN architecture consists of two encoder-decoder components, which define the job operation action policy and the machine action policy for predicting probability distributions over different operations and machines, respectively. We introduce a disjunctive graph representation of FJSP and use a graph neural network to embed the local state encountered during scheduling. The computational experiment results show that the agent can learn a high-quality dis­ patching policy and outperforms handcrafted heuristic dispatching rules in solution quality and meta-heuristic algorithm in running time. Moreover, the results achieved on random and benchmark instances demonstrate that the learned policies have a good generalization performance on real-world instances and significantly larger scale instances with up to 2000 operations.},
	language = {en},
	urldate = {2024-03-26},
	journal = {Expert Systems with Applications},
	author = {Lei, Kun and Guo, Peng and Zhao, Wenchao and Wang, Yi and Qian, Linmao and Meng, Xiangyin and Tang, Liansheng},
	month = nov,
	year = {2022},
	pages = {117796},
	file = {Lei 等 - 2022 - A multi-action deep reinforcement learning framewo.pdf:C\:\\Users\\14532\\Zotero\\storage\\NRK2B926\\Lei 等 - 2022 - A multi-action deep reinforcement learning framewo.pdf:application/pdf},
}

@article{an_flexible_2021,
	title = {Flexible job-shop scheduling and heterogeneous repairman assignment with maintenance time window and employee timetable constraints},
	volume = {186},
	issn = {09574174},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417421010770},
	doi = {10.1016/j.eswa.2021.115693},
	abstract = {This work focuses on a new flexible scheduling problem in the job-shop that considers both maintenance activity and repairman competence under maintenance time window and employee timetable constraints. To deal with this problem, a hybrid multi-objective biogeography-based optimization (HMOBBO) algorithm is proposed, which has the following features: (1) a flexible decoding mechanism that considers maintenance time window and heterogeneous repairman constraints is designed; (2) three calculation methods of habitat suitability index (HSI) are defined; (3) tabu search (TS) algorithm is incorporated into the presented algorithm; and (4) a new offspring population generation mechanism is constructed. In numerical simulation, the parameter setting is firstly analyzed to ensure its robustness for different datasets via comparing the performance of each critical parameter combination. Secondly, different HSIs and migration models are separately compared through multiply running the literature instances, it is shown that fitness function 2 (the reciprocal of the sum of the normalized objective function values) and migration model 1 (constant immigration and linear emigration model) are the most suitable and steady in our experiments. Thirdly, the superiority of HMOBBO is proved by confronting with other six intelligent algorithms. Finally, through contrast variant models, the significance of considering employee timetable and repairman assignment is verified, and the benefits of the integrated optimization model/method are also demonstrated by comparing with hierarchical optimization model/method.},
	language = {en},
	urldate = {2024-03-28},
	journal = {Expert Systems with Applications},
	author = {An, Youjun and Chen, Xiaohui and Li, Yinghe and Zhang, Ji and Jiang, Junwei},
	month = dec,
	year = {2021},
	keywords = {employee time table},
	pages = {115693},
	file = {An 等 - 2021 - Flexible job-shop scheduling and heterogeneous rep.pdf:C\:\\Users\\14532\\Zotero\\storage\\W9QIZYI8\\An 等 - 2021 - Flexible job-shop scheduling and heterogeneous rep.pdf:application/pdf},
}

@article{wu_effective_2021,
	title = {An effective approach for the dual-resource flexible job shop scheduling problem considering loading and unloading},
	volume = {32},
	issn = {1572-8145},
	url = {https://doi.org/10.1007/s10845-020-01697-5},
	doi = {10.1007/s10845-020-01697-5},
	abstract = {Many manufacturing systems need more than one type of resource to co-work with. Commonly studied flexible job shop scheduling problems merely consider the main resource such as machines and ignore the impact of other types of resource. As a result, scheduling solutions may not put into practice. This paper therefore studies the dual resource constrained flexible job shop scheduling problem when loading and unloading time (DRFJSP-LU) of the fixtures is considered. It formulates a multi-objective mathematical model to jointly minimize the makespan and the total setup time. Considering the influence of resource requirement similarity among different operations, we propose a similarity-based scheduling algorithm for setup-time reduction (SSA4STR) and then an improved non-dominated sorting genetic algorithm II (NSGA-II) to optimize the DRFJSP-LU. Experimental results show that the SSA4STR can effectively reduce the loading and unloading time of fixtures while ensuring a level of makespan. The experiments also verify that the scheduling solution with multiple resources has a greater guiding effect on production than the scheduling result with a single resource.},
	language = {en},
	number = {3},
	urldate = {2024-03-28},
	journal = {Journal of Intelligent Manufacturing},
	author = {Wu, Xiuli and Peng, Junjian and Xiao, Xiao and Wu, Shaomin},
	month = mar,
	year = {2021},
	keywords = {Fixture, Flexible job shop scheduling problem, Improved NSGA-II, Resource requirement similarity, Set-up time},
	pages = {707--728},
	file = {Full Text PDF:C\:\\Users\\14532\\Zotero\\storage\\HYIQIBEB\\Wu 等 - 2021 - An effective approach for the dual-resource flexib.pdf:application/pdf},
}

@article{gong_new_2018,
	title = {A new double flexible job-shop scheduling problem integrating processing time, green production, and human factor indicators},
	volume = {174},
	issn = {09596526},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0959652617324952},
	doi = {10.1016/j.jclepro.2017.10.188},
	abstract = {In this paper, we propose an original double ﬂexible job-shop scheduling problem (DFJSP), in which both workers and machines are ﬂexible. Because of the characteristics of double ﬂexibility, DFJSP conforms to practical production better than the ﬂexible job-shop scheduling problem (FJSP). In addition, a multiobjective optimization mathematic model according to the DFJSP is proposed, which is concerned with the processing time indicator that is usually optimized by most existing studies; green production indicators, namely, factors regarding environmental protection; and human factor indicators, which are actual indispensable elements that exist in the production system. Furthermore, ten benchmarks of DFJSP are presented and solved using a newly proposed hybrid genetic algorithm (NHGA). With the proposed NHGA, a new well-designed three-layer chromosome encoding method and some effective crossover and mutation operators are developed. To obtain the best combination of key parameters in NHGA, the Taguchi design of experiment method is used for their evaluation. Finally, comparisons between NHGA and NSGA-II show that the proposed NHGA has advantages in terms of the solving accuracy and efﬁciency of the DFJSP, particularly at a large scale. It would be beneﬁcial to apply our proposed model to the multi-objective optimization of scheduling problems, especially those considering human factor and green production-related indicators.},
	language = {en},
	urldate = {2024-03-28},
	journal = {Journal of Cleaner Production},
	author = {Gong, Guiliang and Deng, Qianwang and Gong, Xuran and Liu, Wei and Ren, Qinghua},
	month = feb,
	year = {2018},
	pages = {560--576},
	file = {Gong 等 - 2018 - A new double flexible job-shop scheduling problem .pdf:C\:\\Users\\14532\\Zotero\\storage\\7E85KUB5\\Gong 等 - 2018 - A new double flexible job-shop scheduling problem .pdf:application/pdf},
}

@article{wang_energy-time_2023,
	title = {Energy-time tradeoffs for remanufacturing system scheduling using an invasive weed optimization algorithm},
	volume = {34},
	issn = {1572-8145},
	url = {https://doi.org/10.1007/s10845-021-01837-5},
	doi = {10.1007/s10845-021-01837-5},
	abstract = {This article studies the scheduling problem for a remanufacturing system with parallel disassembly workstations, parallel flow-shop-type reprocessing lines and parallel reassembly workstations. The problem is formulated as a multi-objective optimization problem which contains both energy consumption and makespan to be addressed using an improved multi-objective invasive weed optimization (MOIWO) algorithm. Two vectors regarding workstation assignment and operation scheduling jointly form a solution. A hybrid initialization strategy is utilized to improve the solution quality and the Sigma method is adopted to rate each solution. A novel seed spatial dispersal mechanism is introduced and four designed mutation operations cooperate to enhance search ability. A group of numerical experiments and a practical case involving the disassembly of transmission devices are carried out and the results validate the effectiveness of the MOIWO algorithm for the considered problem compared with existing methods.},
	language = {en},
	number = {3},
	urldate = {2024-04-02},
	journal = {Journal of Intelligent Manufacturing},
	author = {Wang, Wenjie and Tian, Guangdong and Yuan, Gang and Pham, Duc Truong},
	month = mar,
	year = {2023},
	keywords = {Disassembly, Energy consumption, Multi-objective invasive weed optimization, Remanufacturing system, Shop scheduling},
	pages = {1065--1083},
	file = {Full Text PDF:C\:\\Users\\14532\\Zotero\\storage\\LTBTNJ4Y\\Wang 等 - 2023 - Energy-time tradeoffs for remanufacturing system s.pdf:application/pdf},
}

@misc{infantes_learning_2024,
	title = {Learning to {Solve} {Job} {Shop} {Scheduling} under {Uncertainty}},
	url = {http://arxiv.org/abs/2404.01308},
	abstract = {Job-Shop Scheduling Problem (JSSP) is a combinatorial optimization problem where tasks need to be scheduled on machines in order to minimize criteria such as makespan or delay. To address more realistic scenarios, we associate a probability distribution with the duration of each task. Our objective is to generate a robust schedule, i.e. that minimizes the average makespan. This paper introduces a new approach that leverages Deep Reinforcement Learning (DRL) techniques to search for robust solutions, emphasizing JSSPs with uncertain durations. Key contributions of this research include: (1) advancements in DRL applications to JSSPs, enhancing generalization and scalability, (2) a novel method for addressing JSSPs with uncertain durations. The Wheatley approach, which integrates Graph Neural Networks (GNNs) and DRL, is made publicly available for further research and applications.},
	urldate = {2024-04-08},
	publisher = {arXiv},
	author = {Infantes, Guillaume and Roussel, Stéphanie and Pereira, Pierre and Jacquet, Antoine and Benazera, Emmanuel},
	month = mar,
	year = {2024},
	note = {arXiv:2404.01308 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\14532\\Zotero\\storage\\KU63DKR7\\2404.html:text/html;Full Text PDF:C\:\\Users\\14532\\Zotero\\storage\\ECE6CLJB\\Infantes 等 - 2024 - Learning to Solve Job Shop Scheduling under Uncert.pdf:application/pdf},
}

@misc{echeverria_leveraging_2024,
	title = {Leveraging {Constraint} {Programming} in a {Deep} {Learning} {Approach} for {Dynamically} {Solving} the {Flexible} {Job}-{Shop} {Scheduling} {Problem}},
	url = {http://arxiv.org/abs/2403.09249},
	abstract = {Recent advancements in the flexible job-shop scheduling problem (FJSSP) are primarily based on deep reinforcement learning (DRL) due to its ability to generate high-quality, real-time solutions. However, DRL approaches often fail to fully harness the strengths of existing techniques such as exact methods or constraint programming (CP), which can excel at finding optimal or near-optimal solutions for smaller instances. This paper aims to integrate CP within a deep learning (DL) based methodology, leveraging the benefits of both. In this paper, we introduce a method that involves training a DL model using optimal solutions generated by CP, ensuring the model learns from high-quality data, thereby eliminating the need for the extensive exploration typical in DRL and enhancing overall performance. Further, we integrate CP into our DL framework to jointly construct solutions, utilizing DL for the initial complex stages and transitioning to CP for optimal resolution as the problem is simplified. Our hybrid approach has been extensively tested on three public FJSSP benchmarks, demonstrating superior performance over five state-of-the-art DRL approaches and a widely-used CP solver. Additionally, with the objective of exploring the application to other combinatorial optimization problems, promising preliminary results are presented on applying our hybrid approach to the traveling salesman problem, combining an exact method with a well-known DRL method.},
	urldate = {2024-04-09},
	publisher = {arXiv},
	author = {Echeverria, Imanol and Murua, Maialen and Santana, Roberto},
	month = mar,
	year = {2024},
	note = {arXiv:2403.09249 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:C\:\\Users\\14532\\Zotero\\storage\\48EVLPE2\\2403.html:text/html;Full Text PDF:C\:\\Users\\14532\\Zotero\\storage\\ICSKVWY6\\Echeverria 等 - 2024 - Leveraging Constraint Programming in a Deep Learni.pdf:application/pdf},
}

@misc{corsini_self-labeling_2024,
	title = {Self-{Labeling} the {Job} {Shop} {Scheduling} {Problem}},
	url = {http://arxiv.org/abs/2401.11849},
	abstract = {In this work, we propose a Self-Supervised training strategy specifically designed for combinatorial problems. One of the main obstacles in applying supervised paradigms to such problems is the requirement of expensive target solutions as ground-truth, often produced with costly exact solvers. Inspired by Semi- and Self-Supervised learning, we show that it is possible to easily train generative models by sampling multiple solutions and using the best one according to the problem objective as a pseudo-label. In this way, we iteratively improve the model generation capability by relying only on its self-supervision, completely removing the need for optimality information. We prove the effectiveness of this Self-Labeling strategy on the Job Shop Scheduling (JSP), a complex combinatorial problem that is receiving much attention from the Reinforcement Learning community. We propose a generative model based on the well-known Pointer Network and train it with our strategy. Experiments on two popular benchmarks demonstrate the potential of this approach as the resulting models outperform constructive heuristics and current state-of-the-art Reinforcement Learning proposals.},
	urldate = {2024-04-25},
	publisher = {arXiv},
	author = {Corsini, Andrea and Porrello, Angelo and Calderara, Simone and Dell'Amico, Mauro},
	month = jan,
	year = {2024},
	note = {arXiv:2401.11849 [cs, math]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, G.2, I.2, Mathematics - Combinatorics},
	file = {arXiv.org Snapshot:C\:\\Users\\14532\\Zotero\\storage\\557CNGGM\\2401.html:text/html;Full Text PDF:C\:\\Users\\14532\\Zotero\\storage\\NDEUT5YG\\Corsini 等 - 2024 - Self-Labeling the Job Shop Scheduling Problem.pdf:application/pdf},
}

@inproceedings{li_interpretable_2023,
	title = {Interpretable {Modeling} of {Deep} {Reinforcement} {Learning} {Driven} {Scheduling}},
	url = {http://arxiv.org/abs/2403.16293},
	doi = {10.1109/MASCOTS59514.2023.10387651},
	abstract = {In the field of high-performance computing (HPC), there has been recent exploration into the use of deep reinforcement learning for cluster scheduling (DRL scheduling), which has demonstrated promising outcomes. However, a significant challenge arises from the lack of interpretability in deep neural networks (DNN), rendering them as black-box models to system managers. This lack of model interpretability hinders the practical deployment of DRL scheduling. In this work, we present a framework called IRL (Interpretable Reinforcement Learning) to address the issue of interpretability of DRL scheduling. The core idea is to interpret DNN (i.e., the DRL policy) as a decision tree by utilizing imitation learning. Unlike DNN, decision tree models are non-parametric and easily comprehensible to humans. To extract an effective and efficient decision tree, IRL incorporates the Dataset Aggregation (DAgger) algorithm and introduces the notion of critical state to prune the derived decision tree. Through trace-based experiments, we demonstrate that IRL is capable of converting a black-box DNN policy into an interpretable rulebased decision tree while maintaining comparable scheduling performance. Additionally, IRL can contribute to the setting of rewards in DRL scheduling.},
	urldate = {2024-04-25},
	booktitle = {2023 31st {International} {Symposium} on {Modeling}, {Analysis}, and {Simulation} of {Computer} and {Telecommunication} {Systems} ({MASCOTS})},
	author = {Li, Boyang and Lan, Zhiling and Papka, Michael E.},
	month = oct,
	year = {2023},
	note = {arXiv:2403.16293 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing},
	pages = {1--8},
	file = {arXiv Fulltext PDF:C\:\\Users\\14532\\Zotero\\storage\\YAP4ZESM\\Li 等 - 2023 - Interpretable Modeling of Deep Reinforcement Learn.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\14532\\Zotero\\storage\\JL6HANJW\\2403.html:text/html},
}

@misc{irmouli_genetic_2024,
	title = {Genetic {Algorithm} enhanced by {Deep} {Reinforcement} {Learning} in parent selection mechanism and mutation : {Minimizing} makespan in permutation flow shop scheduling problems},
	shorttitle = {Genetic {Algorithm} enhanced by {Deep} {Reinforcement} {Learning} in parent selection mechanism and mutation},
	url = {http://arxiv.org/abs/2311.05937},
	doi = {10.48550/arXiv.2311.05937},
	abstract = {This paper introduces a reinforcement learning (RL) approach to address the challenges associated with configuring and optimizing genetic algorithms (GAs) for solving difficult combinatorial or non-linear problems. The proposed RL+GA method was specifically tested on the flow shop scheduling problem (FSP). The hybrid algorithm incorporates neural networks (NN) and uses the off-policy method Q-learning or the on-policy method Sarsa(0) to control two key genetic algorithm (GA) operators: parent selection mechanism and mutation. At each generation, the RL agent's action is determining the selection method, the probability of the parent selection and the probability of the offspring mutation. This allows the RL agent to dynamically adjust the selection and mutation based on its learned policy. The results of the study highlight the effectiveness of the RL+GA approach in improving the performance of the primitive GA. They also demonstrate its ability to learn and adapt from population diversity and solution improvements over time. This adaptability leads to improved scheduling solutions compared to static parameter configurations while maintaining population diversity throughout the evolutionary process.},
	urldate = {2024-04-25},
	publisher = {arXiv},
	author = {Irmouli, Maissa and Benazzoug, Nourelhouda and Adimi, Alaa Dania and Rezkellah, Fatma Zohra and Hamzaoui, Imane and Hamitouche, Thanina and Bessedik, Malika and Tayeb, Fatima Si},
	month = jan,
	year = {2024},
	note = {arXiv:2311.05937 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:C\:\\Users\\14532\\Zotero\\storage\\NTPBUP2N\\Irmouli 等 - 2024 - Genetic Algorithm enhanced by Deep Reinforcement L.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\14532\\Zotero\\storage\\2QCYPT9M\\2311.html:text/html},
}

@misc{chen_semantic-aware_2023,
	title = {Semantic-aware {Transmission} {Scheduling}: a {Monotonicity}-driven {Deep} {Reinforcement} {Learning} {Approach}},
	shorttitle = {Semantic-aware {Transmission} {Scheduling}},
	url = {http://arxiv.org/abs/2305.13706},
	abstract = {For cyber-physical systems in the 6G era, semantic communications connecting distributed devices for dynamic control and remote state estimation are required to guarantee application-level performance, not merely focus on communication-centric performance. Semantics here is a measure of the usefulness of information transmissions. Semantic-aware transmission scheduling of a large system often involves a large decision-making space, and the optimal policy cannot be obtained by existing algorithms effectively. In this paper, we first investigate the fundamental properties of the optimal semantic-aware scheduling policy and then develop advanced deep reinforcement learning (DRL) algorithms by leveraging the theoretical guidelines. Our numerical results show that the proposed algorithms can substantially reduce training time and enhance training performance compared to benchmark algorithms.},
	urldate = {2024-04-25},
	publisher = {arXiv},
	author = {Chen, Jiazheng and Liu, Wanchun and Quevedo, Daniel and Li, Yonghui and Vucetic, Branka},
	month = sep,
	year = {2023},
	note = {arXiv:2305.13706 [cs, eess, math]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Information Theory, Electrical Engineering and Systems Science - Signal Processing, Electrical Engineering and Systems Science - Systems and Control},
	file = {arXiv.org Snapshot:C\:\\Users\\14532\\Zotero\\storage\\6B9CS58E\\2305.html:text/html;Full Text PDF:C\:\\Users\\14532\\Zotero\\storage\\XP2VS6HF\\Chen 等 - 2023 - Semantic-aware Transmission Scheduling a Monotoni.pdf:application/pdf},
}

@incollection{jarvis_cuda_2015,
	address = {Cham},
	title = {A {CUDA} {Implementation} of the {High} {Performance} {Conjugate} {Gradient} {Benchmark}},
	volume = {8966},
	isbn = {978-3-319-17247-7 978-3-319-17248-4},
	url = {https://link.springer.com/10.1007/978-3-319-17248-4_4},
	abstract = {The High Performance Conjugate Gradient (HPCG) benchmark has been recently proposed as a complement to the High Performance Linpack (HPL) benchmark currently used to rank supercomputers in the Top500 list. This new benchmark solves a large sparse linear system using a multigrid preconditioned conjugate gradient (PCG) algorithm. The PCG algorithm contains the computational and communication patterns prevalent in the numerical solution of partial diﬀerential equations and is designed to better represent modern application workloads which rely more heavily on memory system and network performance than HPL. GPU accelerated supercomputers have proved to be very eﬀective, especially with regard to power eﬃciency, for accelerating compute intensive applications like HPL. This paper will present the details of a CUDA implementation of HPCG, and the results obtained at full scale on the largest GPU supercomputers available: the Cray XK7 at ORNL and the Cray XC30 at CSCS. The results indicate that GPU accelerated supercomputers are also very eﬀective for this type of workload.},
	language = {en},
	urldate = {2024-05-06},
	booktitle = {High {Performance} {Computing} {Systems}. {Performance} {Modeling}, {Benchmarking}, and {Simulation}},
	publisher = {Springer International Publishing},
	author = {Phillips, Everett and Fatica, Massimiliano},
	editor = {Jarvis, Stephen A. and Wright, Steven A. and Hammond, Simon D.},
	year = {2015},
	doi = {10.1007/978-3-319-17248-4_4},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {68--84},
	file = {Phillips 和 Fatica - 2015 - A CUDA Implementation of the High Performance Conj.pdf:C\:\\Users\\14532\\Zotero\\storage\\SCC4QCC3\\Phillips 和 Fatica - 2015 - A CUDA Implementation of the High Performance Conj.pdf:application/pdf},
}

@article{duan_comprehensive_nodate,
	title = {A {Comprehensive} {Study} on {Large}-{Scale} {Graph} {Training}: {Benchmarking} and {Rethinking}},
	abstract = {Large-scale graph training is a notoriously challenging problem for graph neural networks (GNNs). Due to the nature of evolving graph structures into the training process, vanilla GNNs usually fail to scale up, limited by the GPU memory space. Up to now, though numerous scalable GNN architectures have been proposed, we still lack a comprehensive survey and fair benchmark of this reservoir to ﬁnd the rationale for designing scalable GNNs. To this end, we ﬁrst systematically formulate the representative methods of large-scale graph training into several branches and further establish a fair and consistent benchmark for them by a greedy hyperparameter searching. In addition, regarding efﬁciency, we theoretically evaluate the time and space complexity of various branches and empirically compare them w.r.t GPU memory usage, throughput, and convergence. Furthermore, We analyze the pros and cons for various branches of scalable GNNs and then present a new ensembling training manner, named EnGCN, to address the existing issues. Remarkably, our proposed method has achieved new state-ofthe-art (SOTA) performance on large-scale datasets. Our code is available at https://github.com/VITA-Group/Large\_Scale\_GCN\_Benchmarking.},
	language = {en},
	author = {Duan, Keyu and Liu, Zirui and Wang, Peihao and Zheng, Wenqing and Zhou, Kaixiong and Chen, Tianlong and Hu, Xia and Wang, Zhangyang},
	keywords = {l2t},
	file = {Duan 等 - A Comprehensive Study on Large-Scale Graph Trainin.pdf:C\:\\Users\\14532\\Zotero\\storage\\QKFLQ3LL\\Duan 等 - A Comprehensive Study on Large-Scale Graph Trainin.pdf:application/pdf},
}

@article{duan_comprehensive_nodate-1,
	title = {A {Comprehensive} {Study} on {Large}-{Scale} {Graph} {Training}: {Benchmarking} and {Rethinking}},
	abstract = {Large-scale graph training is a notoriously challenging problem for graph neural networks (GNNs). Due to the nature of evolving graph structures into the training process, vanilla GNNs usually fail to scale up, limited by the GPU memory space. Up to now, though numerous scalable GNN architectures have been proposed, we still lack a comprehensive survey and fair benchmark of this reservoir to ﬁnd the rationale for designing scalable GNNs. To this end, we ﬁrst systematically formulate the representative methods of large-scale graph training into several branches and further establish a fair and consistent benchmark for them by a greedy hyperparameter searching. In addition, regarding efﬁciency, we theoretically evaluate the time and space complexity of various branches and empirically compare them w.r.t GPU memory usage, throughput, and convergence. Furthermore, We analyze the pros and cons for various branches of scalable GNNs and then present a new ensembling training manner, named EnGCN, to address the existing issues. Remarkably, our proposed method has achieved new state-ofthe-art (SOTA) performance on large-scale datasets. Our code is available at https://github.com/VITA-Group/Large\_Scale\_GCN\_Benchmarking.},
	language = {en},
	author = {Duan, Keyu and Liu, Zirui and Wang, Peihao and Zheng, Wenqing and Zhou, Kaixiong and Chen, Tianlong and Hu, Xia and Wang, Zhangyang},
	file = {Duan 等 - A Comprehensive Study on Large-Scale Graph Trainin.pdf:C\:\\Users\\14532\\Zotero\\storage\\7UCJ53DL\\Duan 等 - A Comprehensive Study on Large-Scale Graph Trainin.pdf:application/pdf},
}

@misc{cao_learning_2023,
	title = {Learning {Large} {Graph} {Property} {Prediction} via {Graph} {Segment} {Training}},
	url = {http://arxiv.org/abs/2305.12322},
	abstract = {Learning to predict properties of a large graph is challenging because each prediction requires the knowledge of an entire graph, while the amount of memory available during training is bounded. Here we propose Graph Segment Training (GST), a general framework that utilizes a divide-and-conquer approach to allow learning large graph property prediction with a constant memory footprint. GST first divides a large graph into segments and then backpropagates through only a few segments sampled per training iteration. We refine the GST paradigm by introducing a historical embedding table to efficiently obtain embeddings for segments not sampled for backpropagation. To mitigate the staleness of historical embeddings, we design two novel techniques. First, we finetune the prediction head to fix the input distribution shift. Second, we introduce Stale Embedding Dropout to drop some stale embeddings during training to reduce bias. We evaluate our complete method GST+EFD (with all the techniques together) on two large graph property prediction benchmarks: MalNet and TpuGraphs. Our experiments show that GST+EFD is both memory-efficient and fast, while offering a slight boost on test accuracy over a typical full graph training regime.},
	language = {en},
	urldate = {2024-05-24},
	publisher = {arXiv},
	author = {Cao, Kaidi and Phothilimthana, Phitchaya Mangpo and Abu-El-Haija, Sami and Zelle, Dustin and Zhou, Yanqi and Mendis, Charith and Leskovec, Jure and Perozzi, Bryan},
	month = nov,
	year = {2023},
	note = {arXiv:2305.12322 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Social and Information Networks},
	file = {Cao 等 - 2023 - Learning Large Graph Property Prediction via Graph.pdf:C\:\\Users\\14532\\Zotero\\storage\\B8CSC9Z9\\Cao 等 - 2023 - Learning Large Graph Property Prediction via Graph.pdf:application/pdf},
}

@misc{cao_learning_2023-1,
	title = {Learning {Large} {Graph} {Property} {Prediction} via {Graph} {Segment} {Training}},
	url = {http://arxiv.org/abs/2305.12322},
	abstract = {Learning to predict properties of a large graph is challenging because each prediction requires the knowledge of an entire graph, while the amount of memory available during training is bounded. Here we propose Graph Segment Training (GST), a general framework that utilizes a divide-and-conquer approach to allow learning large graph property prediction with a constant memory footprint. GST first divides a large graph into segments and then backpropagates through only a few segments sampled per training iteration. We refine the GST paradigm by introducing a historical embedding table to efficiently obtain embeddings for segments not sampled for backpropagation. To mitigate the staleness of historical embeddings, we design two novel techniques. First, we finetune the prediction head to fix the input distribution shift. Second, we introduce Stale Embedding Dropout to drop some stale embeddings during training to reduce bias. We evaluate our complete method GST+EFD (with all the techniques together) on two large graph property prediction benchmarks: MalNet and TpuGraphs. Our experiments show that GST+EFD is both memory-efficient and fast, while offering a slight boost on test accuracy over a typical full graph training regime.},
	language = {en},
	urldate = {2024-05-24},
	publisher = {arXiv},
	author = {Cao, Kaidi and Phothilimthana, Phitchaya Mangpo and Abu-El-Haija, Sami and Zelle, Dustin and Zhou, Yanqi and Mendis, Charith and Leskovec, Jure and Perozzi, Bryan},
	month = nov,
	year = {2023},
	note = {arXiv:2305.12322 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Social and Information Networks},
	file = {Cao 等 - 2023 - Learning Large Graph Property Prediction via Graph.pdf:C\:\\Users\\14532\\Zotero\\storage\\4MDPIWMN\\Cao 等 - 2023 - Learning Large Graph Property Prediction via Graph.pdf:application/pdf},
}

@inproceedings{ye_gnngbdt-guided_2023,
	title = {{GNN}\&{GBDT}-{Guided} {Fast} {Optimizing} {Framework} for {Large}-scale {Integer} {Programming}},
	url = {https://proceedings.mlr.press/v202/ye23e.html},
	abstract = {The latest two-stage optimization framework based on graph neural network (GNN) and large neighborhood search (LNS) is the most popular framework in solving large-scale integer programs (IPs). However, the framework can not effectively use the embedding spatial information in GNN and still highly relies on large-scale solvers in LNS, resulting in the scale of IP being limited by the ability of the current solver and performance bottlenecks. To handle these issues, this paper presents a GNN\&GBDT-guided fast optimizing framework for large-scale IPs that only uses a small-scale optimizer to solve large-scale IPs efficiently. Specifically, the proposed framework can be divided into three stages: Multi-task GNN Embedding to generate the embedding space, GBDT Prediction to effectively use the embedding spatial information, and Neighborhood Optimization to solve large-scale problems fast using the small-scale optimizer. Extensive experiments show that the proposed framework can solve IPs with millions of scales and surpass SCIP and Gurobi in the specified wall-clock time using only a small-scale optimizer with 30\% of the problem size. It also shows that the proposed framework can save 99\% of running time in achieving the same solution quality as SCIP, which verifies the effectiveness and efficiency of the proposed framework in solving large-scale IPs.},
	language = {en},
	urldate = {2024-05-24},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Ye, Huigen and Xu, Hua and Wang, Hongyan and Wang, Chengming and Jiang, Yu},
	month = jul,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {39864--39878},
	file = {Full Text PDF:C\:\\Users\\14532\\Zotero\\storage\\GTD2A9S6\\Ye 等 - 2023 - GNN&GBDT-Guided Fast Optimizing Framework for Larg.pdf:application/pdf},
}

@article{ye_light-milpopt_2024,
	title = {{LIGHT}-{MILPOPT}: {SOLVING} {LARGE}-{SCALE} {MIXED} {INTEGER} {LINEAR} {PROGRAMS} {WITH} {LIGHTWEIGHT} {OPTIMIZER} {AND} {SMALL}-{SCALE} {TRAINING} {DATASET}},
	abstract = {Machine Learning (ML)-based optimization approaches emerge as a promising technique for solving large-scale Mixed Integer Linear Programs (MILPs). However, existing ML-based frameworks suffer from high model computation complexity, weak problem reduction, and reliance on large-scale optimizers and large training datasets, resulting in performance bottlenecks for large-scale MILPs. This paper proposes Light-MILPopt, a lightweight large-scale optimization framework that only uses a lightweight optimizer and small training dataset to solve large-scale MILPs. Specifically, Light-MILPopt can be divided into four stages: Problem Formulation for problem division to reduce model computational costs, Model-based Initial Solution Prediction for predicting and constructing the initial solution using a small-scale training dataset, Problem Reduction for both variable and constraint reduction, and Data-driven Optimization for current solution improvement employing a lightweight optimizer. Experimental evaluations on four large-scale benchmark MILPs and a real-world case study demonstrate that LightMILPopt, leveraging a lightweight optimizer and small training dataset, outperforms the state-of-the-art ML-based optimization framework and advanced largescale solvers (e.g. Gurobi, SCIP). The results and further analyses substantiate the ML-based framework’s feasibility and effectiveness in solving large-scale MILPs.},
	language = {en},
	author = {Ye, Huigen and Xu, Hua and Wang, Hongyan},
	year = {2024},
	file = {Ye 等 - 2024 - LIGHT-MILPOPT SOLVING LARGE-SCALE MIXED INTEGER L.pdf:C\:\\Users\\14532\\Zotero\\storage\\A29ZZGQG\\Ye 等 - 2024 - LIGHT-MILPOPT SOLVING LARGE-SCALE MIXED INTEGER L.pdf:application/pdf},
}

@misc{chen_learning_2024,
	title = {Learning to optimize: {A} tutorial for continuous and mixed-integer optimization},
	shorttitle = {Learning to optimize},
	url = {http://arxiv.org/abs/2405.15251},
	abstract = {Learning to Optimize (L2O) stands at the intersection of traditional optimization and machine learning, utilizing the capabilities of machine learning to enhance conventional optimization techniques. As realworld optimization problems frequently share common structures, L2O provides a tool to exploit these structures for better or faster solutions. This tutorial dives deep into L2O techniques, introducing how to accelerate optimization algorithms, promptly estimate the solutions, or even reshape the optimization problem itself, making it more adaptive to real-world applications. By considering the prerequisites for successful applications of L2O and the structure of the optimization problems at hand, this tutorial provides a comprehensive guide for practitioners and researchers alike.},
	language = {en},
	urldate = {2024-05-27},
	publisher = {arXiv},
	author = {Chen, Xiaohan and Liu, Jialin and Yin, Wotao},
	month = may,
	year = {2024},
	note = {arXiv:2405.15251 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control},
	file = {2405.pdf:C\:\\Users\\14532\\Zotero\\storage\\53U6IIFY\\2405.pdf:application/pdf},
}

@misc{chen_learning_2024-1,
	title = {Learning to optimize: {A} tutorial for continuous and mixed-integer optimization},
	shorttitle = {Learning to optimize},
	url = {http://arxiv.org/abs/2405.15251},
	abstract = {Learning to Optimize (L2O) stands at the intersection of traditional optimization and machine learning, utilizing the capabilities of machine learning to enhance conventional optimization techniques. As realworld optimization problems frequently share common structures, L2O provides a tool to exploit these structures for better or faster solutions. This tutorial dives deep into L2O techniques, introducing how to accelerate optimization algorithms, promptly estimate the solutions, or even reshape the optimization problem itself, making it more adaptive to real-world applications. By considering the prerequisites for successful applications of L2O and the structure of the optimization problems at hand, this tutorial provides a comprehensive guide for practitioners and researchers alike.},
	language = {en},
	urldate = {2024-05-27},
	publisher = {arXiv},
	author = {Chen, Xiaohan and Liu, Jialin and Yin, Wotao},
	month = may,
	year = {2024},
	note = {arXiv:2405.15251 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control},
	file = {Chen 等 - 2024 - Learning to optimize A tutorial for continuous an.pdf:C\:\\Users\\14532\\Zotero\\storage\\MKG66NBD\\Chen 等 - 2024 - Learning to optimize A tutorial for continuous an.pdf:application/pdf},
}

@article{li_deep_2023,
	title = {Deep {Reinforcement} {Learning} for {Online} {Assortment} {Customization}: {A} {Data}-{Driven} {Approach}},
	issn = {1556-5068},
	shorttitle = {Deep {Reinforcement} {Learning} for {Online} {Assortment} {Customization}},
	url = {https://www.ssrn.com/abstract=4413200},
	doi = {10.2139/ssrn.4413200},
	abstract = {When a retailer has limited inventory and is operating on a periodic selling schedule, it is important to have a variety of products available for each customer. To maximize revenue over the long term, an optimal assortment policy is required that takes into account the complex purchasing behaviors of customers whose arrival order and preferences are unknown. By analyzing historical customer arrival and transaction data, we propose a data-driven approach for dynamic assortment planning. To address the challenge of online assortment customization, we utilize a Markov decision process (MDP) framework and employ a model-free deep reinforcement learning (DRL) approach to learn a policy that is nearly optimal. Our method involves using a specialized deep learning model called Gated-DNN to create assortments while adhering to constraints, and a modified version of the Advantage Actor-Critic (A2C) algorithm to adjust the parameters of the Gated-DNN model. The updates to the model’s parameters are done by simulated interactions with historical sequences of customer arrivals. The feedback we receive from simulated customers can take any form and should match the historical transaction data as closely as possible to ensure the effectiveness of the policy we learn. To evaluate the effectiveness of our approach, we conduct simulations using both a synthetic data set generated with a pre-determined customer type distribution and choice model, as well as a real-world data set. Our extensive experiments demonstrate that our approach produces significantly higher long-term revenue compared to existing methods and remains robust under various conditions. We also demonstrate that our approach can be easily adapted to a more general problem that includes reusable products, where customers return purchased items after a period of time. In further experiments, we find that our approach performs best under various usage time distributions.},
	language = {en},
	urldate = {2024-05-27},
	journal = {SSRN Electronic Journal},
	author = {Li, Tao and Wang, Chenhao and Wang, Yao and Tang, Shaojie},
	year = {2023},
	file = {Li 等 - 2023 - Deep Reinforcement Learning for Online Assortment .pdf:C\:\\Users\\14532\\Zotero\\storage\\WDZGP8XX\\Li 等 - 2023 - Deep Reinforcement Learning for Online Assortment .pdf:application/pdf},
}

@article{banchio_market_2022,
	title = {Market {Design} for {AI} {Algorithms}},
	volume = {20},
	language = {en},
	number = {2},
	journal = {ACM SIGecom Exchanges},
	author = {Banchio, Martino},
	year = {2022},
	file = {Banchio - 2022 - Market Design for AI Algorithms.pdf:C\:\\Users\\14532\\Zotero\\storage\\N52MSEUE\\Banchio - 2022 - Market Design for AI Algorithms.pdf:application/pdf},
}

@article{devanur_online_2011,
	title = {Online algorithms with stochastic input},
	volume = {10},
	issn = {1551-9031},
	url = {https://dl.acm.org/doi/10.1145/1998549.1998558},
	doi = {10.1145/1998549.1998558},
	abstract = {The design and analysis of
              online
              algorithms, where the input to the algorithm is revealed over time and the algorithm has to make decisions immediately without knowing the future input, has received a revived interest in the last few years primarily due to their application to online advertising. The canonical problem is the
              Adwords
              problem, which is motivated by the problem of optimally allocating ad slots on search queries to budget constrained advertisers. It involves simplifications that ignore certain aspects of the actual way this allocation is done. For instance, it assumes a "first-price" pay-per-impression scheme, ignoring the game theoretic aspects, and considers only one slot per query. To be precise, the Adwords problem is as follows.},
	language = {en},
	number = {2},
	urldate = {2024-05-28},
	journal = {ACM SIGecom Exchanges},
	author = {Devanur, Nikhil R.},
	month = jun,
	year = {2011},
	pages = {40--49},
	file = {Devanur - 2011 - Online algorithms with stochastic input.pdf:C\:\\Users\\14532\\Zotero\\storage\\9Z8FPNGW\\Devanur - 2011 - Online algorithms with stochastic input.pdf:application/pdf},
}

@article{dasaratha_network_2021,
	title = {Network structure and social learning},
	volume = {19},
	issn = {1551-9031},
	url = {https://dl.acm.org/doi/10.1145/3505156.3505163},
	doi = {10.1145/3505156.3505163},
	abstract = {We describe results from Dasaratha and He [DH21a] and Dasaratha and He [DH20] about how network structure influences social learning outcomes. These papers share a tractable sequential model that lets us compare learning dynamics across networks. With Bayesian agents, incomplete networks can generate informational confounding that makes learning arbitrarily inefficient. With naive agents, related forces can lead to mislearning.},
	language = {en},
	number = {2},
	urldate = {2024-05-31},
	journal = {ACM SIGecom Exchanges},
	author = {Dasaratha, Krishna and He, Kevin},
	month = nov,
	year = {2021},
	pages = {62--67},
	file = {Dasaratha 和 He - 2021 - Network structure and social learning.pdf:C\:\\Users\\14532\\Zotero\\storage\\KYJVYWNY\\Dasaratha 和 He - 2021 - Network structure and social learning.pdf:application/pdf},
}

@article{wang_deep_2023,
	title = {Deep {Reinforcement} {Learning} for {Sequential} {Targeting}},
	volume = {69},
	issn = {0025-1909, 1526-5501},
	url = {https://pubsonline.informs.org/doi/10.1287/mnsc.2022.4621},
	doi = {10.1287/mnsc.2022.4621},
	abstract = {Deep reinforcement learning (DRL) has opened up many unprecedented opportu­ nities in revolutionizing the digital marketing field. In this study, we designed a DRL-based personalized targeting strategy in a sequential setting. We show that the strategy is able to address three important challenges of sequential targeting: (1) forward looking (balancing between a firm’s current revenue and future revenues), (2) earning while learning (maximizing profits while continuously learning through exploration-exploitation), and (3) scalability (cop­ ing with a high-dimensional state and policy space). We illustrate this through a novel design of a DRL-based artificial intelligence (AI) agent. To better adapt DRL to complex consumer behavior dimensions, we proposed a quantization-based uncertainty learning heuristic for effi­ cient exploration-exploitation. Our policy evaluation results through simulation suggest that the proposed DRL agent generates 26.75\% more long-term revenues than can the non-DRL approaches on average and learns 76.92\% faster than the second fastest model among all bench­ marks. Further, in order to better understand the potential underlying mechanisms, we con­ ducted multiple interpretability analyses to explain the patterns of learned optimal policy at both the individual and population levels. Our findings provide important managerialrelevant and theory-consistent insights. For instance, consecutive price promotions at the begin­ ning can capture price-sensitive consumers’ immediate attention, whereas carefully spaced nonpromotional “cooldown” periods between price promotions can allow consumers to adjust their reference points. Additionally, consideration of future revenues is necessary from a longterm horizon, but weighing the future too much can also dampen revenues. In addition, analy­ ses of heterogeneous treatment effects suggest that the optimal promotion sequence pattern highly varies across the consumer engagement stages. Overall, our study results demonstrate DRL’s potential to optimize these strategies’ combination to maximize long-term revenues.},
	language = {en},
	number = {9},
	urldate = {2024-06-04},
	journal = {Management Science},
	author = {Wang, Wen and Li, Beibei and Luo, Xueming and Wang, Xiaoyi},
	month = sep,
	year = {2023},
	pages = {5439--5460},
	file = {Wang 等 - 2023 - Deep Reinforcement Learning for Sequential Targeti.pdf:C\:\\Users\\14532\\Zotero\\storage\\G25URY32\\Wang 等 - 2023 - Deep Reinforcement Learning for Sequential Targeti.pdf:application/pdf},
}

@article{adelman_dynamic_2007,
	title = {Dynamic {Bid} {Prices} in {Revenue} {Management}},
	language = {en},
	journal = {Operations Research},
	author = {Adelman, Daniel},
	year = {2007},
	file = {Adelman - 2007 - Dynamic Bid Prices in Revenue Management.pdf:C\:\\Users\\14532\\Zotero\\storage\\2ICDR4FU\\Adelman - 2007 - Dynamic Bid Prices in Revenue Management.pdf:application/pdf},
}

@article{erdelyi_dynamic_2010,
	title = {A {Dynamic} {Programming} {Decomposition} {Method} for {Making} {Overbooking} {Decisions} {Over} an {Airline} {Network}},
	volume = {22},
	issn = {1091-9856, 1526-5528},
	url = {https://pubsonline.informs.org/doi/10.1287/ijoc.1090.0359},
	doi = {10.1287/ijoc.1090.0359},
	abstract = {In this paper, we develop a revenue management model to jointly make the capacity allocation and overbooking decisions over an airline network. Our approach begins with the dynamic programming formulation of the capacity allocation and overbooking problem and uses an approximation strategy to decompose the dynamic programming formulation by the flight legs. This decomposition idea opens up the possibility of obtaining approximate solutions by concentrating on one flight leg at a time, but the capacity allocation and overbooking problem that takes place over a single flight leg still turns out to be intractable. We use a state aggregation approach to obtain high-quality solutions to the single-leg problem. Overall, our model constructs separable approximations to the value functions, which can be used to make the capacity allocation and overbooking decisions for the whole airline network. Computational experiments indicate that our model performs significantly better than a variety of benchmark strategies from the literature.},
	language = {en},
	number = {3},
	urldate = {2024-06-04},
	journal = {INFORMS Journal on Computing},
	author = {Erdelyi, Alexander and Topaloglu, Huseyin},
	month = aug,
	year = {2010},
	pages = {443--456},
	file = {Erdelyi 和 Topaloglu - 2010 - A Dynamic Programming Decomposition Method for Mak.pdf:C\:\\Users\\14532\\Zotero\\storage\\PV33XIDS\\Erdelyi 和 Topaloglu - 2010 - A Dynamic Programming Decomposition Method for Mak.pdf:application/pdf},
}

@misc{rampasek_recipe_2023,
	title = {Recipe for a {General}, {Powerful}, {Scalable} {Graph} {Transformer}},
	url = {http://arxiv.org/abs/2205.12454},
	abstract = {We propose a recipe on how to build a general, powerful, scalable (GPS) graph Transformer with linear complexity and state-of-the-art results on a diverse set of benchmarks. Graph Transformers (GTs) have gained popularity in the ﬁeld of graph representation learning with a variety of recent publications but they lack a common foundation about what constitutes a good positional or structural encoding, and what differentiates them. In this paper, we summarize the different types of encodings with a clearer deﬁnition and categorize them as being local, global or relative. The prior GTs are constrained to small graphs with a few hundred nodes, here we propose the ﬁrst architecture with a complexity linear in the number of nodes and edges O(N + E) by decoupling the local real-edge aggregation from the fully-connected Transformer. We argue that this decoupling does not negatively affect the expressivity, with our architecture being a universal function approximator on graphs. Our GPS recipe consists of choosing 3 main ingredients: (i) positional/structural encoding, (ii) local message-passing mechanism, and (iii) global attention mechanism. We provide a modular framework GRAPHGPS1 that supports multiple types of encodings and that provides efﬁciency and scalability both in small and large graphs. We test our architecture on 16 benchmarks and show highly competitive results in all of them, show-casing the empirical beneﬁts gained by the modularity and the combination of different strategies.},
	language = {en},
	urldate = {2024-06-07},
	publisher = {arXiv},
	author = {Rampášek, Ladislav and Galkin, Mikhail and Dwivedi, Vijay Prakash and Luu, Anh Tuan and Wolf, Guy and Beaini, Dominique},
	month = jan,
	year = {2023},
	note = {arXiv:2205.12454 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Rampášek 等 - 2023 - Recipe for a General, Powerful, Scalable Graph Tra.pdf:C\:\\Users\\14532\\Zotero\\storage\\7L6V9EGN\\Rampášek 等 - 2023 - Recipe for a General, Powerful, Scalable Graph Tra.pdf:application/pdf},
}

@misc{rampasek_recipe_2023-1,
	title = {Recipe for a {General}, {Powerful}, {Scalable} {Graph} {Transformer}},
	url = {http://arxiv.org/abs/2205.12454},
	abstract = {We propose a recipe on how to build a general, powerful, scalable (GPS) graph Transformer with linear complexity and state-of-the-art results on a diverse set of benchmarks. Graph Transformers (GTs) have gained popularity in the ﬁeld of graph representation learning with a variety of recent publications but they lack a common foundation about what constitutes a good positional or structural encoding, and what differentiates them. In this paper, we summarize the different types of encodings with a clearer deﬁnition and categorize them as being local, global or relative. The prior GTs are constrained to small graphs with a few hundred nodes, here we propose the ﬁrst architecture with a complexity linear in the number of nodes and edges O(N + E) by decoupling the local real-edge aggregation from the fully-connected Transformer. We argue that this decoupling does not negatively affect the expressivity, with our architecture being a universal function approximator on graphs. Our GPS recipe consists of choosing 3 main ingredients: (i) positional/structural encoding, (ii) local message-passing mechanism, and (iii) global attention mechanism. We provide a modular framework GRAPHGPS1 that supports multiple types of encodings and that provides efﬁciency and scalability both in small and large graphs. We test our architecture on 16 benchmarks and show highly competitive results in all of them, show-casing the empirical beneﬁts gained by the modularity and the combination of different strategies.},
	language = {en},
	urldate = {2024-06-07},
	publisher = {arXiv},
	author = {Rampášek, Ladislav and Galkin, Mikhail and Dwivedi, Vijay Prakash and Luu, Anh Tuan and Wolf, Guy and Beaini, Dominique},
	month = jan,
	year = {2023},
	note = {arXiv:2205.12454 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Rampášek 等 - 2023 - Recipe for a General, Powerful, Scalable Graph Tra.pdf:C\:\\Users\\14532\\Zotero\\storage\\M7PH5T7D\\Rampášek 等 - 2023 - Recipe for a General, Powerful, Scalable Graph Tra.pdf:application/pdf},
}

@misc{rafailov_direct_2023,
	title = {Direct {Preference} {Optimization}: {Your} {Language} {Model} is {Secretly} a {Reward} {Model}},
	shorttitle = {Direct {Preference} {Optimization}},
	url = {http://arxiv.org/abs/2305.18290},
	abstract = {While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.},
	language = {en},
	urldate = {2024-06-10},
	publisher = {arXiv},
	author = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D. and Finn, Chelsea},
	month = dec,
	year = {2023},
	note = {arXiv:2305.18290 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Rafailov 等 - 2023 - Direct Preference Optimization Your Language Mode.pdf:C\:\\Users\\14532\\Zotero\\storage\\YXCAQRU7\\Rafailov 等 - 2023 - Direct Preference Optimization Your Language Mode.pdf:application/pdf},
}

@misc{hosny_automatic_2023,
	title = {Automatic {MILP} {Solver} {Configuration} {By} {Learning} {Problem} {Similarities}},
	url = {http://arxiv.org/abs/2307.00670},
	abstract = {A large number of real-world optimization problems can be formulated as Mixed Integer Linear Programs (MILP). MILP solvers expose numerous configuration parameters to control their internal algorithms. Solutions, and their associated costs or runtimes, are significantly affected by the choice of the configuration parameters, even when problem instances have the same number of decision variables and constraints. On one hand, using the default solver configuration leads to suboptimal solutions. On the other hand, searching and evaluating a large number of configurations for every problem instance is time-consuming and, in some cases, infeasible. In this study, we aim to predict configuration parameters for unseen problem instances that yield lower-cost solutions without the time overhead of searching-and-evaluating configurations at the solving time. Toward that goal, we first investigate the cost correlation of MILP problem instances that come from the same distribution when solved using different configurations. We show that instances that have similar costs using one solver configuration also have similar costs using another solver configuration in the same runtime environment. After that, we present a methodology based on Deep Metric Learning to learn MILP similarities that correlate with their final solutions' costs. At inference time, given a new problem instance, it is first projected into the learned metric space using the trained model, and configuration parameters are instantly predicted using previously-explored configurations from the nearest neighbor instance in the learned embedding space. Empirical results on real-world problem benchmarks show that our method predicts configuration parameters that improve solutions' costs by up to 38\% compared to existing approaches.},
	language = {en},
	urldate = {2024-06-14},
	publisher = {arXiv},
	author = {Hosny, Abdelrahman and Reda, Sherief},
	month = jul,
	year = {2023},
	note = {arXiv:2307.00670 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control},
	file = {Hosny 和 Reda - 2023 - Automatic MILP Solver Configuration By Learning Pr.pdf:C\:\\Users\\14532\\Zotero\\storage\\DN4WXR7S\\Hosny 和 Reda - 2023 - Automatic MILP Solver Configuration By Learning Pr.pdf:application/pdf},
}

@article{roughgarden_cs364a_nodate-8,
	title = {{CS364A}: {Algorithmic} {Game} {Theory} {Lecture} \#10: {Kidney} {Exchange} and {Stable} {Matching}},
	language = {en},
	author = {Roughgarden, Tim},
	file = {Roughgarden - CS364A Algorithmic Game Theory Lecture #10 Kidne.pdf:C\:\\Users\\14532\\Zotero\\storage\\SNA87UFW\\Roughgarden - CS364A Algorithmic Game Theory Lecture #10 Kidne.pdf:application/pdf},
}

@article{roughgarden_cs364a_nodate-9,
	title = {{CS364A}: {Algorithmic} {Game} {Theory} {Lecture} \#11: {Selﬁsh} {Routing} and the {Price} of {Anarchy}},
	language = {en},
	author = {Roughgarden, Tim},
	file = {Roughgarden - CS364A Algorithmic Game Theory Lecture #11 Selﬁs.pdf:C\:\\Users\\14532\\Zotero\\storage\\7S7TS4XJ\\Roughgarden - CS364A Algorithmic Game Theory Lecture #11 Selﬁs.pdf:application/pdf},
}

@article{roughgarden_cs364a_nodate-10,
	title = {{CS364A}: {Algorithmic} {Game} {Theory} {Lecture} \#12: {More} on {Selﬁsh} {Routing}},
	language = {en},
	author = {Roughgarden, Tim},
	file = {Roughgarden - CS364A Algorithmic Game Theory Lecture #12 More .pdf:C\:\\Users\\14532\\Zotero\\storage\\YWB49KSD\\Roughgarden - CS364A Algorithmic Game Theory Lecture #12 More .pdf:application/pdf},
}

@article{roughgarden_cs364a_nodate-11,
	title = {{CS364A}: {Algorithmic} {Game} {Theory} {Lecture} \#13: {Potential} {Games}; {A} {Hierarchy} of {Equilibria}},
	language = {en},
	author = {Roughgarden, Tim},
	file = {Roughgarden - CS364A Algorithmic Game Theory Lecture #13 Poten.pdf:C\:\\Users\\14532\\Zotero\\storage\\45HDKPR6\\Roughgarden - CS364A Algorithmic Game Theory Lecture #13 Poten.pdf:application/pdf},
}

@article{roughgarden_cs364a_nodate-12,
	title = {{CS364A}: {Algorithmic} {Game} {Theory} {Lecture} \#14: {Robust} {Price}-of-{Anarchy} {Bounds} in {Smooth} {Games}},
	language = {en},
	author = {Roughgarden, Tim},
	file = {Roughgarden - CS364A Algorithmic Game Theory Lecture #14 Robus.pdf:C\:\\Users\\14532\\Zotero\\storage\\8HQ2XKQX\\Roughgarden - CS364A Algorithmic Game Theory Lecture #14 Robus.pdf:application/pdf},
}

@article{roughgarden_cs364a_nodate-13,
	title = {{CS364A}: {Algorithmic} {Game} {Theory} {Lecture} \#15: {Best}-{Case} and {Strong} {Nash} {Equilibria}},
	language = {en},
	author = {Roughgarden, Tim},
	file = {Roughgarden - CS364A Algorithmic Game Theory Lecture #15 Best-.pdf:C\:\\Users\\14532\\Zotero\\storage\\5EELGUQU\\Roughgarden - CS364A Algorithmic Game Theory Lecture #15 Best-.pdf:application/pdf},
}

@article{roughgarden_cs364a_nodate-14,
	title = {{CS364A}: {Algorithmic} {Game} {Theory} {Lecture} \#16: {Best}-{Response} {Dynamics}},
	language = {en},
	author = {Roughgarden, Tim},
	file = {Roughgarden - CS364A Algorithmic Game Theory Lecture #16 Best-.pdf:C\:\\Users\\14532\\Zotero\\storage\\FVTPHI36\\Roughgarden - CS364A Algorithmic Game Theory Lecture #16 Best-.pdf:application/pdf},
}

@article{roughgarden_cs364a_nodate-15,
	title = {{CS364A}: {Algorithmic} {Game} {Theory} {Lecture} \#17: {No}-{Regret} {Dynamics}},
	language = {en},
	author = {Roughgarden, Tim},
	file = {Roughgarden - CS364A Algorithmic Game Theory Lecture #17 No-Re.pdf:C\:\\Users\\14532\\Zotero\\storage\\CGRT6ZU5\\Roughgarden - CS364A Algorithmic Game Theory Lecture #17 No-Re.pdf:application/pdf},
}

@article{roughgarden_cs364a_nodate-16,
	title = {{CS364A}: {Algorithmic} {Game} {Theory} {Lecture} \#18: {From} {External} {Regret} to {Swap} {Regret} and the {Minimax} {Theorem}},
	language = {en},
	author = {Roughgarden, Tim},
	file = {Roughgarden - CS364A Algorithmic Game Theory Lecture #18 From .pdf:C\:\\Users\\14532\\Zotero\\storage\\7RSTBEH3\\Roughgarden - CS364A Algorithmic Game Theory Lecture #18 From .pdf:application/pdf},
}

@article{roughgarden_cs364a_nodate-17,
	title = {{CS364A}: {Algorithmic} {Game} {Theory} {Lecture} \#19: {Pure} {Nash} {Equilibria} and {PLS}-{Completeness}},
	language = {en},
	author = {Roughgarden, Tim},
	file = {Roughgarden - CS364A Algorithmic Game Theory Lecture #19 Pure .pdf:C\:\\Users\\14532\\Zotero\\storage\\MVPWFEYF\\Roughgarden - CS364A Algorithmic Game Theory Lecture #19 Pure .pdf:application/pdf},
}

@article{roughgarden_cs364a_nodate-18,
	title = {{CS364A}: {Algorithmic} {Game} {Theory} {Lecture} \#20: {Mixed} {Nash} {Equilibria} and {PPAD}-{Completeness}},
	language = {en},
	author = {Roughgarden, Tim},
	file = {Roughgarden - CS364A Algorithmic Game Theory Lecture #20 Mixed.pdf:C\:\\Users\\14532\\Zotero\\storage\\TBAAXW8I\\Roughgarden - CS364A Algorithmic Game Theory Lecture #20 Mixed.pdf:application/pdf},
}

@misc{peyre_computational_2020,
	title = {Computational {Optimal} {Transport}},
	url = {http://arxiv.org/abs/1803.00567},
	abstract = {Optimal transport (OT) theory can be informally described using the words of the French mathematician Gaspard Monge (1746-1818): A worker with a shovel in hand has to move a large pile of sand lying on a construction site. The goal of the worker is to erect with all that sand a target pile with a prescribed shape (for example, that of a giant sand castle). Naturally, the worker wishes to minimize her total effort, quantified for instance as the total distance or time spent carrying shovelfuls of sand. Mathematicians interested in OT cast that problem as that of comparing two probability distributions, two different piles of sand of the same volume. They consider all of the many possible ways to morph, transport or reshape the first pile into the second, and associate a "global" cost to every such transport, using the "local" consideration of how much it costs to move a grain of sand from one place to another. Recent years have witnessed the spread of OT in several fields, thanks to the emergence of approximate solvers that can scale to sizes and dimensions that are relevant to data sciences. Thanks to this newfound scalability, OT is being increasingly used to unlock various problems in imaging sciences (such as color or texture processing), computer vision and graphics (for shape manipulation) or machine learning (for regression, classification and density fitting). This short book reviews OT with a bias toward numerical methods and their applications in data sciences, and sheds lights on the theoretical properties of OT that make it particularly useful for some of these applications.},
	language = {en},
	urldate = {2024-06-25},
	publisher = {arXiv},
	author = {Peyré, Gabriel and Cuturi, Marco},
	month = mar,
	year = {2020},
	note = {arXiv:1803.00567 [stat]},
	keywords = {Statistics - Machine Learning},
	file = {1803.pdf:C\:\\Users\\14532\\Zotero\\storage\\HDC55GXV\\1803.pdf:application/pdf},
}

@article{santambrogio_optimal_nodate,
	title = {Optimal {Transport} for {Applied} {Mathematicians} – {Calculus} of {Variations}, {PDEs} and {Modeling}},
	language = {en},
	author = {Santambrogio, Filippo},
	file = {Santambrogio - Optimal Transport for Applied Mathematicians – Cal.pdf:C\:\\Users\\14532\\Zotero\\storage\\7KCBIGT5\\Santambrogio - Optimal Transport for Applied Mathematicians – Cal.pdf:application/pdf},
}

@article{santambrogio_optimal_nodate-1,
	title = {Optimal {Transport} for {Applied} {Mathematicians} – {Calculus} of {Variations}, {PDEs} and {Modeling}},
	language = {en},
	author = {Santambrogio, Filippo},
	file = {Santambrogio - Optimal Transport for Applied Mathematicians – Cal.pdf:C\:\\Users\\14532\\Zotero\\storage\\CAY3986Z\\Santambrogio - Optimal Transport for Applied Mathematicians – Cal.pdf:application/pdf},
}

@misc{tassel_reinforcement_2021,
	title = {A {Reinforcement} {Learning} {Environment} {For} {Job}-{Shop} {Scheduling}},
	url = {http://arxiv.org/abs/2104.03760},
	abstract = {Scheduling is a fundamental task occurring in various automated systems applications, e.g., optimal schedules for machines on a job shop allow for a reduction of production costs and waste. Nevertheless, ﬁnding such schedules is often intractable and cannot be achieved by Combinatorial Optimization Problem (COP) methods within a given time limit. Recent advances of Deep Reinforcement Learning (DRL) in learning complex behavior enable new COP application possibilities. This paper presents an efﬁcient DRL environment for Job-Shop Scheduling – an important problem in the ﬁeld. Furthermore, we design a meaningful and compact state representation as well as a novel, simple dense reward function, closely related to the sparse make-span minimization criteria used by COP methods. We demonstrate that our approach signiﬁcantly outperforms existing DRL methods on classic benchmark instances, coming close to state-of-the-art COP approaches.},
	language = {en},
	urldate = {2024-07-04},
	publisher = {arXiv},
	author = {Tassel, Pierre and Gebser, Martin and Schekotihin, Konstantin},
	month = apr,
	year = {2021},
	note = {arXiv:2104.03760 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Tassel 等 - 2021 - A Reinforcement Learning Environment For Job-Shop .pdf:C\:\\Users\\14532\\Zotero\\storage\\66JU4IJY\\Tassel 等 - 2021 - A Reinforcement Learning Environment For Job-Shop .pdf:application/pdf},
}

@inproceedings{belmamoune_solving_2022,
	address = {BUCHAREST, Romania},
	series = {Studies in {Computational} {Intelligence}},
	title = {Solving a {Job} {Shop} {Scheduling} {Problem} {Using} {Q}-{Learning} {Algorithm}},
	volume = {1083},
	url = {https://hal.science/hal-03951519},
	doi = {10.1007/978-3-031-24291-5_16},
	abstract = {Job Shop Scheduling Problem (JSSP) is among the combinatorial optimization and Non-Deterministic Polynomial-time (NP) problems. Researchers have contributed in this area using several methods, among the methods we have machine learning algorithms, more precisely Reinforcement Learning (RL). The reason why the scientists resort to RL is the adequacy of the algorithm for this type of problem. The results of the RL approach tend toward optimal or nearoptimal solutions. In this paper, we deal with the JSSP, using the RL algorithm, more specifically a Q-learning algorithm. We propose a new representation of the state of the environment. We introduce two evaluations of the agent using two different methods. The actions selected by the agent are the dispatching rules. Finally, we compared the results obtained by the approach with the literature.},
	urldate = {2024-07-04},
	booktitle = {{SOHOMA} 2022 , 12th international workshop on service oriented, holonic and multi-agent manufacturing systems for industry of the future},
	publisher = {Springer International Publishing},
	author = {Belmamoune, Manal Abir and Ghomri, Latéfa and Yahouni, Zakaria},
	month = sep,
	year = {2022},
	keywords = {dispatching rules, Job Shop Scheduling Problem, Job Shop Scheduling Problem Reinforcement Learning dispatching rules Q-learning, Q-learning, Reinforcement Learning},
	pages = {196--209},
	file = {Full Text PDF:C\:\\Users\\14532\\Zotero\\storage\\JS7CB8XR\\Belmamoune 等 - 2022 - Solving a Job Shop Scheduling Problem Using Q-Lear.pdf:application/pdf},
}

@misc{moerland_model-based_2022,
	title = {Model-based {Reinforcement} {Learning}: {A} {Survey}},
	shorttitle = {Model-based {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2006.16712},
	doi = {10.48550/arXiv.2006.16712},
	abstract = {Sequential decision making, commonly formalized as Markov Decision Process (MDP) optimization, is a important challenge in artificial intelligence. Two key approaches to this problem are reinforcement learning (RL) and planning. This paper presents a survey of the integration of both fields, better known as model-based reinforcement learning. Model-based RL has two main steps. First, we systematically cover approaches to dynamics model learning, including challenges like dealing with stochasticity, uncertainty, partial observability, and temporal abstraction. Second, we present a systematic categorization of planning-learning integration, including aspects like: where to start planning, what budgets to allocate to planning and real data collection, how to plan, and how to integrate planning in the learning and acting loop. After these two sections, we also discuss implicit model-based RL as an end-to-end alternative for model learning and planning, and we cover the potential benefits of model-based RL. Along the way, the survey also draws connections to several related RL fields, like hierarchical RL and transfer learning. Altogether, the survey presents a broad conceptual overview of the combination of planning and learning for MDP optimization.},
	urldate = {2024-08-26},
	publisher = {arXiv},
	author = {Moerland, Thomas M. and Broekens, Joost and Plaat, Aske and Jonker, Catholijn M.},
	month = mar,
	year = {2022},
	note = {arXiv:2006.16712 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\14532\\Zotero\\storage\\WAWT4XAM\\Moerland 等 - 2022 - Model-based Reinforcement Learning A Survey.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\14532\\Zotero\\storage\\NQC7FUCX\\2006.html:text/html},
}

@misc{tancik_fourier_2020,
	title = {Fourier {Features} {Let} {Networks} {Learn} {High} {Frequency} {Functions} in {Low} {Dimensional} {Domains}},
	url = {http://arxiv.org/abs/2006.10739},
	abstract = {We show that passing input points through a simple Fourier feature mapping enables a multilayer perceptron (MLP) to learn high-frequency functions in lowdimensional problem domains. These results shed light on recent advances in computer vision and graphics that achieve state-of-the-art results by using MLPs to represent complex 3D objects and scenes. Using tools from the neural tangent kernel (NTK) literature, we show that a standard MLP fails to learn high frequencies both in theory and in practice. To overcome this spectral bias, we use a Fourier feature mapping to transform the effective NTK into a stationary kernel with a tunable bandwidth. We suggest an approach for selecting problem-speciﬁc Fourier features that greatly improves the performance of MLPs for low-dimensional regression tasks relevant to the computer vision and graphics communities.},
	language = {en},
	urldate = {2024-08-29},
	publisher = {arXiv},
	author = {Tancik, Matthew and Srinivasan, Pratul P. and Mildenhall, Ben and Fridovich-Keil, Sara and Raghavan, Nithin and Singhal, Utkarsh and Ramamoorthi, Ravi and Barron, Jonathan T. and Ng, Ren},
	month = jun,
	year = {2020},
	note = {arXiv:2006.10739 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Tancik 等 - 2020 - Fourier Features Let Networks Learn High Frequency.pdf:C\:\\Users\\14532\\Zotero\\storage\\66RSRUBF\\Tancik 等 - 2020 - Fourier Features Let Networks Learn High Frequency.pdf:application/pdf},
}

@misc{li_pdhg-unrolled_2024,
	title = {{PDHG}-{Unrolled} {Learning}-to-{Optimize} {Method} for {Large}-{Scale} {Linear} {Programming}},
	url = {http://arxiv.org/abs/2406.01908},
	abstract = {Solving large-scale linear programming (LP) problems is an important task in various areas such as communication networks, power systems, finance, and logistics. Recently, two distinct approaches have emerged to expedite LP solving: (i) First-order methods (FOMs); (ii) Learning to optimize (L2O). In this work, we propose a FOMunrolled neural network (NN) called PDHG-Net, and propose a two-stage L2O method to solve large-scale LP problems. The new architecture PDHG-Net is designed by unrolling the recently emerged PDHG method into a neural network, combined with channel-expansion techniques borrowed from graph neural networks. We prove that the proposed PDHG-Net can recover PDHG algorithm, thus can approximate optimal solutions of LP instances with a polynomial number of neurons. We propose a two-stage inference approach: first use PDHG-Net to generate an approximate solution, and then apply the PDHG algorithm to further improve the solution. Experiments show that our approach can significantly accelerate LP solving, achieving up to a 3× speedup compared to FOMs for large-scale LP problems.},
	language = {en},
	urldate = {2024-09-02},
	publisher = {arXiv},
	author = {Li, Bingheng and Yang, Linxin and Chen, Yupeng and Wang, Senmiao and Chen, Qian and Mao, Haitao and Ma, Yao and Wang, Akang and Ding, Tian and Tang, Jiliang and Sun, Ruoyu},
	month = jun,
	year = {2024},
	note = {arXiv:2406.01908 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control},
	file = {Li 等 - 2024 - PDHG-Unrolled Learning-to-Optimize Method for Larg.pdf:C\:\\Users\\14532\\Zotero\\storage\\L7MJTZHI\\Li 等 - 2024 - PDHG-Unrolled Learning-to-Optimize Method for Larg.pdf:application/pdf},
}

@misc{shehzad_graph_2024,
	title = {Graph {Transformers}: {A} {Survey}},
	shorttitle = {Graph {Transformers}},
	url = {http://arxiv.org/abs/2407.09777},
	abstract = {Graph transformers are a recent advancement in machine learning, offering a new class of neural network models for graph-structured data. The synergy between transformers and graph learning demonstrates strong performance and versatility across various graph-related tasks. This survey provides an in-depth review of recent progress and challenges in graph transformer research. We begin with foundational concepts of graphs and transformers. We then explore design perspectives of graph transformers, focusing on how they integrate graph inductive biases and graph attention mechanisms into the transformer architecture. Furthermore, we propose a taxonomy classifying graph transformers based on depth, scalability, and pre-training strategies, summarizing key principles for effective development of graph transformer models. Beyond technical analysis, we discuss the applications of graph transformer models for node-level, edge-level, and graph-level tasks, exploring their potential in other application scenarios as well. Finally, we identify remaining challenges in the field, such as scalability and efficiency, generalization and robustness, interpretability and explainability, dynamic and complex graphs, as well as data quality and diversity, charting future directions for graph transformer research.},
	language = {en},
	urldate = {2024-09-02},
	publisher = {arXiv},
	author = {Shehzad, Ahsan and Xia, Feng and Abid, Shagufta and Peng, Ciyuan and Yu, Shuo and Zhang, Dongyu and Verspoor, Karin},
	month = jul,
	year = {2024},
	note = {arXiv:2407.09777 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, 68T07, 68T05, 68U01, I.2.6},
	file = {Shehzad 等 - 2024 - Graph Transformers A Survey.pdf:C\:\\Users\\14532\\Zotero\\storage\\HT8MBZJV\\Shehzad 等 - 2024 - Graph Transformers A Survey.pdf:application/pdf},
}

@misc{soro_diffusion-based_2024,
	title = {Diffusion-based {Neural} {Network} {Weights} {Generation}},
	url = {http://arxiv.org/abs/2402.18153},
	abstract = {Transfer learning is a topic of significant interest in recent deep learning research because it enables faster convergence and improved performance on new tasks. While the performance of transfer learning depends on the similarity of the source data to the target data, it is costly to train a model on a large number of datasets. Therefore, pretrained models are generally blindly selected with the hope that they will achieve good performance on the given task. To tackle such suboptimality of the pretrained models, we propose an efficient and adaptive transfer learning scheme through dataset-conditioned pretrained weights sampling. Specifically, we use a latent diffusion model with a variational autoencoder that can reconstruct the neural network weights, to learn the distribution of a set of pretrained weights conditioned on each dataset for transfer learning on unseen datasets. By learning the distribution of a neural network on a variety pretrained models, our approach enables adaptive sampling weights for unseen datasets achieving faster convergence and reaching competitive performance.},
	language = {en},
	urldate = {2024-09-16},
	publisher = {arXiv},
	author = {Soro, Bedionita and Andreis, Bruno and Lee, Hayeon and Chong, Song and Hutter, Frank and Hwang, Sung Ju},
	month = feb,
	year = {2024},
	note = {arXiv:2402.18153 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Soro 等 - 2024 - Diffusion-based Neural Network Weights Generation.pdf:C\:\\Users\\14532\\Zotero\\storage\\QWYQ94XG\\Soro 等 - 2024 - Diffusion-based Neural Network Weights Generation.pdf:application/pdf},
}

@misc{lu_practical_2024,
	title = {A {Practical} and {Optimal} {First}-{Order} {Method} for {Large}-{Scale} {Convex} {Quadratic} {Programming}},
	url = {http://arxiv.org/abs/2311.07710},
	abstract = {Convex quadratic programming (QP) is an important class of optimization problem with wide applications in practice. The classic QP solvers are based on either simplex or barrier method, both of which suffer from the scalability issue because their computational bottleneck is solving linear equations. In this paper, we design and analyze a first-order method for QP, called restarted accelerated primal-dual hybrid gradient (rAPDHG), whose computational bottleneck is matrix-vector multiplication. We show that rAPDHG has a linear convergence rate to an optimal solution when solving QP, and the obtained linear rate is optimal among a wide class of primal-dual methods. Furthermore, we connect the linear rate with a sharpness constant of the KKT system of QP, which is a standard quantity to measure the hardness of a continuous optimization problem. Numerical experiments demonstrate that both restarts and acceleration can significantly improve the performance of the algorithm. Lastly, we present PDQP.jl, an opensource solver based on rAPDHG that can be run on both GPU and CPU. With a numerical comparison with SCS and OSQP on standard QP benchmark sets and large-scale synthetic QP instances, we demonstrate the effectiveness of rAPDHG for solving QP.},
	language = {en},
	urldate = {2024-09-25},
	publisher = {arXiv},
	author = {Lu, Haihao and Yang, Jinwen},
	month = may,
	year = {2024},
	note = {arXiv:2311.07710 [math]},
	keywords = {Mathematics - Optimization and Control},
	file = {Lu 和 Yang - 2024 - A Practical and Optimal First-Order Method for Lar.pdf:C\:\\Users\\14532\\Zotero\\storage\\VD8U79U9\\Lu 和 Yang - 2024 - A Practical and Optimal First-Order Method for Lar.pdf:application/pdf},
}

@article{sun_precompact_2018,
	title = {Precompact convergence of the nonconvex {Primal}–{Dual} {Hybrid} {Gradient} algorithm},
	volume = {330},
	issn = {03770427},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0377042717303886},
	doi = {10.1016/j.cam.2017.07.037},
	abstract = {The Primal–Dual Hybrid Gradient (PDHG) algorithm is a powerful algorithm used quite frequently in recent years for solving saddle-point optimization problems. The classical application considers convex functions, and it is well studied in literature. In this paper, we consider the convergence of an alternative formulation of the PDHG algorithm in the nonconvex case under the precompact assumption. The proofs are based on the Kurdyka–Ł ojasiewic functions, that cover a wide range of problems. A simple numerical experiment illustrates the convergence properties.},
	language = {en},
	urldate = {2024-09-27},
	journal = {Journal of Computational and Applied Mathematics},
	author = {Sun, Tao and Barrio, Roberto and Cheng, Lizhi and Jiang, Hao},
	month = mar,
	year = {2018},
	pages = {15--27},
	file = {Sun 等 - 2018 - Precompact convergence of the nonconvex Primal–Dua.pdf:C\:\\Users\\14532\\Zotero\\storage\\W7FKZ4XK\\Sun 等 - 2018 - Precompact convergence of the nonconvex Primal–Dua.pdf:application/pdf},
}

@article{clason_acceleration_2019,
	title = {Acceleration and {Global} {Convergence} of a {First}-{Order} {Primal}-{Dual} {Method} for {Nonconvex} {Problems}},
	volume = {29},
	issn = {1052-6234, 1095-7189},
	url = {https://epubs.siam.org/doi/10.1137/18M1170194},
	doi = {10.1137/18M1170194},
	abstract = {The primal-dual hybrid gradient method, modiﬁed (PDHGM, also known as the Chambolle–Pock method), has proved very successful for convex optimization problems involving linear operators arising in image processing and inverse problems. In this paper, we analyze an extension to nonconvex problems that arise if the operator is nonlinear. Based on the idea of testing, we derive new step-length parameter conditions for the convergence in inﬁnite-dimensional Hilbert spaces and provide acceleration rules for suitably (locally and/or partially) monotone problems. Importantly, we prove linear convergence rates as well as global convergence in certain cases. We demonstrate the eﬃcacy of these step-length rules for PDE-constrained optimization problems.},
	language = {en},
	number = {1},
	urldate = {2024-09-27},
	journal = {SIAM Journal on Optimization},
	author = {Clason, Christian and Mazurenko, Stanislav and Valkonen, Tuomo},
	month = jan,
	year = {2019},
	pages = {933--963},
	file = {Clason 等 - 2019 - Acceleration and Global Convergence of a First-Ord.pdf:C\:\\Users\\14532\\Zotero\\storage\\N5PZDI99\\Clason 等 - 2019 - Acceleration and Global Convergence of a First-Ord.pdf:application/pdf},
}

@article{mollenhoff_primal-dual_2015,
	title = {The {Primal}-{Dual} {Hybrid} {Gradient} {Method} for {Semiconvex} {Splittings}},
	volume = {8},
	issn = {1936-4954},
	url = {http://epubs.siam.org/doi/10.1137/140976601},
	doi = {10.1137/140976601},
	abstract = {This paper deals with the analysis of a recent reformulation of the primal-dual hybrid gradient method, which allows one to apply it to nonconvex regularizers. Particularly, it investigates variational problems for which the energy to be minimized can be written as G(u) + F (Ku), where G is convex, F is semiconvex, and K is a linear operator. We study the method and prove convergence in the case where the nonconvexity of F is compensated for by the strong convexity of G. The convergence proof yields an interesting requirement for the choice of algorithm parameters, which we show to be not only suﬃcient, but also necessary. Additionally, we show boundedness of the iterates under much weaker conditions. Finally, in several numerical experiments we demonstrate eﬀectiveness and convergence of the algorithm beyond the theoretical guarantees.},
	language = {en},
	number = {2},
	urldate = {2024-09-27},
	journal = {SIAM Journal on Imaging Sciences},
	author = {Möllenhoff, Thomas and Strekalovskiy, Evgeny and Moeller, Michael and Cremers, Daniel},
	month = jan,
	year = {2015},
	pages = {827--857},
	file = {Möllenhoff 等 - 2015 - The Primal-Dual Hybrid Gradient Method for Semicon.pdf:C\:\\Users\\14532\\Zotero\\storage\\CDDY2UX2\\Möllenhoff 等 - 2015 - The Primal-Dual Hybrid Gradient Method for Semicon.pdf:application/pdf},
}

@article{hermans_qpalm_2022,
	title = {{QPALM}: a proximal augmented lagrangian method for nonconvex quadratic programs},
	volume = {14},
	issn = {1867-2949, 1867-2957},
	shorttitle = {{QPALM}},
	url = {https://link.springer.com/10.1007/s12532-022-00218-0},
	doi = {10.1007/s12532-022-00218-0},
	abstract = {We propose QPALM, a nonconvex quadratic programming (QP) solver based on the proximal augmented Lagrangian method. This method solves a sequence of inner subproblems which can be enforced to be strongly convex and which therefore admit a unique solution. The resulting steps are shown to be equivalent to inexact proximal point iterations on the extended-real-valued cost function, which allows for a fairly simple analysis where convergence to a stationary point at an R-linear rate is shown. The QPALM algorithm solves the subproblems iteratively using semismooth Newton directions and an exact linesearch. The former can be computed efﬁciently in most iterations by making use of suitable factorization update routines, while the latter requires the zero of a monotone, one-dimensional, piecewise afﬁne function. QPALM is implemented in open-source C code, with tailored linear algebra routines for the factorization in a self-written package LADEL. The resulting implementation is shown to be extremely robust in numerical simulations, solving all of the Maros-Meszaros problems and ﬁnding a stationary point for most of the nonconvex QPs in the Cutest test set. Furthermore, it is shown to be competitive against state-of-the-art convex QP solvers in typical QPs arising from application domains such as portfolio optimization and model predictive control. As such, QPALM strikes a unique balance between solving both easy and hard problems efﬁciently.},
	language = {en},
	number = {3},
	urldate = {2024-09-28},
	journal = {Mathematical Programming Computation},
	author = {Hermans, Ben and Themelis, Andreas and Patrinos, Panagiotis},
	month = sep,
	year = {2022},
	pages = {497--541},
	file = {Hermans 等 - 2022 - QPALM a proximal augmented lagrangian method for .pdf:C\:\\Users\\14532\\Zotero\\storage\\LJ5EQWW6\\Hermans 等 - 2022 - QPALM a proximal augmented lagrangian method for .pdf:application/pdf},
}

@misc{grimmer_landscape_2021,
	title = {The {Landscape} of the {Proximal} {Point} {Method} for {Nonconvex}-{Nonconcave} {Minimax} {Optimization}},
	url = {http://arxiv.org/abs/2006.08667},
	abstract = {Minimax optimization has become a central tool in machine learning with applications in robust optimization, reinforcement learning, GANs, etc. These applications are often nonconvexnonconcave, but the existing theory is unable to identify and deal with the fundamental diﬃculties this poses. In this paper, we study the classic proximal point method (PPM) applied to nonconvexnonconcave minimax problems. We ﬁnd that a classic generalization of the Moreau envelope by Attouch and Wets provides key insights. Critcally, we show this envelope not only smooths the objective but can convexify and concavify it based on the level of interaction present between the minimizing and maximizing variables. From this, we identify three distinct regions of nonconvex-nonconcave problems. When interaction is suﬃciently strong, we derive global linear convergence guarantees. Conversely when the interaction is fairly weak, we derive local linear convergence guarantees with a proper initialization. Between these two settings, we show that PPM may diverge or converge to a limit cycle.},
	language = {en},
	urldate = {2024-10-03},
	publisher = {arXiv},
	author = {Grimmer, Benjamin and Lu, Haihao and Worah, Pratik and Mirrokni, Vahab},
	month = apr,
	year = {2021},
	note = {arXiv:2006.08667 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control, 65K05, 65K10, 90C26, 90C15, 90C30},
	file = {Grimmer 等 - 2021 - The Landscape of the Proximal Point Method for Non.pdf:C\:\\Users\\14532\\Zotero\\storage\\24YERSDQ\\Grimmer 等 - 2021 - The Landscape of the Proximal Point Method for Non.pdf:application/pdf},
}

@misc{grimmer_landscape_2021-1,
	title = {The {Landscape} of the {Proximal} {Point} {Method} for {Nonconvex}-{Nonconcave} {Minimax} {Optimization}},
	url = {http://arxiv.org/abs/2006.08667},
	abstract = {Minimax optimization has become a central tool in machine learning with applications in robust optimization, reinforcement learning, GANs, etc. These applications are often nonconvexnonconcave, but the existing theory is unable to identify and deal with the fundamental diﬃculties this poses. In this paper, we study the classic proximal point method (PPM) applied to nonconvexnonconcave minimax problems. We ﬁnd that a classic generalization of the Moreau envelope by Attouch and Wets provides key insights. Critcally, we show this envelope not only smooths the objective but can convexify and concavify it based on the level of interaction present between the minimizing and maximizing variables. From this, we identify three distinct regions of nonconvex-nonconcave problems. When interaction is suﬃciently strong, we derive global linear convergence guarantees. Conversely when the interaction is fairly weak, we derive local linear convergence guarantees with a proper initialization. Between these two settings, we show that PPM may diverge or converge to a limit cycle.},
	language = {en},
	urldate = {2024-10-03},
	publisher = {arXiv},
	author = {Grimmer, Benjamin and Lu, Haihao and Worah, Pratik and Mirrokni, Vahab},
	month = apr,
	year = {2021},
	note = {arXiv:2006.08667 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control, 65K05, 65K10, 90C26, 90C15, 90C30},
	file = {Grimmer 等 - 2021 - The Landscape of the Proximal Point Method for Non.pdf:C\:\\Users\\14532\\Zotero\\storage\\DQU29BK3\\Grimmer 等 - 2021 - The Landscape of the Proximal Point Method for Non.pdf:application/pdf},
}

@misc{hajizadeh_linear_2023,
	title = {On the {Linear} {Convergence} of {Extra}-{Gradient} {Methods} for {Nonconvex}-{Nonconcave} {Minimax} {Problems}},
	url = {http://arxiv.org/abs/2201.06167},
	abstract = {Recently, minimax optimization received renewed focus due to modern applications in machine learning, robust optimization, and reinforcement learning. The scale of these applications naturally leads to the use of ﬁrst-order methods. However, the nonconvexities and nonconcavities present in these problems, prevents the application of typical Gradient Descent-Ascent, which is known to diverge even in bilinear problems. Recently, it was shown that the Proximal Point Method (PPM) converges linearly for a family of nonconvex-nonconcave problems. In this paper, we study the convergence of a damped version of Extra-Gradient Method (EGM) which avoids potentially costly proximal computations, only relying on gradient evaluation. We show that EGM converges linearly for smooth minimax optimization problem satisfying the same nonconvex-nonconcave condition needed by PPM.},
	language = {en},
	urldate = {2024-10-03},
	publisher = {arXiv},
	author = {Hajizadeh, Saeed and Lu, Haihao and Grimmer, Benjamin},
	month = mar,
	year = {2023},
	note = {arXiv:2201.06167 [math]},
	keywords = {Mathematics - Optimization and Control},
	file = {Hajizadeh 等 - 2023 - On the Linear Convergence of Extra-Gradient Method.pdf:C\:\\Users\\14532\\Zotero\\storage\\MXCREI2C\\Hajizadeh 等 - 2023 - On the Linear Convergence of Extra-Gradient Method.pdf:application/pdf},
}

@misc{sun_difusco_2023,
	title = {{DIFUSCO}: {Graph}-based {Diffusion} {Solvers} for {Combinatorial} {Optimization}},
	shorttitle = {{DIFUSCO}},
	url = {http://arxiv.org/abs/2302.08224},
	abstract = {Neural network-based Combinatorial Optimization (CO) methods have shown promising results in solving various NP-complete (NPC) problems without relying on hand-crafted domain knowledge. This paper broadens the current scope of neural solvers for NPC problems by introducing a new graph-based diffusion framework, namely DIFUSCO. Our framework casts NPC problems as discrete \{0, 1\}-vector optimization problems and leverages graph-based denoising diffusion models to generate high-quality solutions. We investigate two types of diffusion models with Gaussian and Bernoulli noise, respectively, and devise an effective inference schedule to enhance the solution quality. We evaluate our methods on two well-studied NPC combinatorial optimization problems: Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS). Experimental results show that DIFUSCO strongly outperforms the previous state-of-the-art neural solvers, improving the performance gap between ground-truth and neural solvers from 1.76\% to 0.46\% on TSP-500, from 2.46\% to 1.17\% on TSP-1000, and from 3.19\% to 2.58\% on TSP-10000. For the MIS problem, DIFUSCO outperforms the previous state-of-the-art neural solver on the challenging SATLIB benchmark.},
	language = {en},
	urldate = {2024-10-04},
	publisher = {arXiv},
	author = {Sun, Zhiqing and Yang, Yiming},
	month = dec,
	year = {2023},
	note = {arXiv:2302.08224 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Sun 和 Yang - 2023 - DIFUSCO Graph-based Diffusion Solvers for Combina.pdf:C\:\\Users\\14532\\Zotero\\storage\\PMBDYKBU\\Sun 和 Yang - 2023 - DIFUSCO Graph-based Diffusion Solvers for Combina.pdf:application/pdf},
}

@misc{ma_efficient_2024,
	title = {Efficient {Combinatorial} {Optimization} via {Heat} {Diffusion}},
	url = {http://arxiv.org/abs/2403.08757},
	abstract = {Combinatorial optimization problems are widespread but inherently challenging due to their discrete nature. The primary limitation of existing methods is that they can only access a small fraction of the solution space at each iteration, resulting in limited efficiency for searching the global optimal. To overcome this challenge, diverging from conventional efforts of expanding the solver’s search scope, we focus on enabling information to actively propagate to the solver through heat diffusion. By transforming the target function while preserving its optima, heat diffusion facilitates information flow from distant regions to the solver, providing more efficient navigation. Utilizing heat diffusion, we propose a framework for solving general combinatorial optimization problems. The proposed methodology demonstrates superior performance across a range of the most challenging and widely encountered combinatorial optimizations. Echoing recent advancements in harnessing thermodynamics for generative artificial intelligence, our study further reveals its significant potential in advancing combinatorial optimization. The codebase of our study is available in https://github.com/AwakerMhy/HeO.},
	language = {en},
	urldate = {2024-10-04},
	publisher = {arXiv},
	author = {Ma, Hengyuan and Lu, Wenlian and Feng, Jianfeng},
	month = sep,
	year = {2024},
	note = {arXiv:2403.08757 [physics, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Combinatorics, Physics - Applied Physics},
	file = {Ma 等 - 2024 - Efficient Combinatorial Optimization via Heat Diff.pdf:C\:\\Users\\14532\\Zotero\\storage\\VL345E76\\Ma 等 - 2024 - Efficient Combinatorial Optimization via Heat Diff.pdf:application/pdf},
}

@misc{terpin_trust_2022,
	title = {Trust {Region} {Policy} {Optimization} with {Optimal} {Transport} {Discrepancies}: {Duality} and {Algorithm} for {Continuous} {Actions}},
	shorttitle = {Trust {Region} {Policy} {Optimization} with {Optimal} {Transport} {Discrepancies}},
	url = {http://arxiv.org/abs/2210.11137},
	abstract = {Policy Optimization (PO) algorithms have been proven particularly suited to handle the high-dimensionality of real-world continuous control tasks. In this context, Trust Region Policy Optimization methods represent a popular approach to stabilize the policy updates. These usually rely on the Kullback-Leibler (KL) divergence to limit the change in the policy. The Wasserstein distance represents a natural alternative, in place of the KL divergence, to deﬁne trust regions or to regularize the objective function. However, state-of-the-art works either resort to its approximations or do not provide an algorithm for continuous state-action spaces, reducing the applicability of the method. In this paper, we explore optimal transport discrepancies (which include the Wasserstein distance) to deﬁne trust regions, and we propose a novel algorithm – Optimal Transport Trust Region Policy Optimization (OT-TRPO) – for continuous state-action spaces. We circumvent the inﬁnite-dimensional optimization problem for PO by providing a one-dimensional dual reformulation for which strong duality holds. We then analytically derive the optimal policy update given the solution of the dual problem. This way, we bypass the computation of optimal transport costs and of optimal transport maps, which we implicitly characterize by solving the dual formulation. Finally, we provide an experimental evaluation of our approach across various control tasks. Our results show that optimal transport discrepancies can offer an advantage over state-of-theart approaches.},
	language = {en},
	urldate = {2024-10-09},
	publisher = {arXiv},
	author = {Terpin, Antonio and Lanzetti, Nicolas and Yardim, Batuhan and Dörfler, Florian and Ramponi, Giorgia},
	month = oct,
	year = {2022},
	note = {arXiv:2210.11137 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control},
	file = {Terpin 等 - 2022 - Trust Region Policy Optimization with Optimal Tran.pdf:C\:\\Users\\14532\\Zotero\\storage\\CR8URQJ7\\Terpin 等 - 2022 - Trust Region Policy Optimization with Optimal Tran.pdf:application/pdf},
}

@misc{pasula_lagrangian_2020,
	title = {Lagrangian {Duality} in {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2007.09998},
	abstract = {Although duality is used extensively in certain ﬁelds, such as supervised learning in machine learning, it has been much less explored in others, such as reinforcement learning (RL). In this paper, we show how duality is involved in a variety of RL work, from that which spearheaded the ﬁeld, such as Richard Bellman’s value iteration, to that which was done within just the past few years yet has already had signiﬁcant impact, such as TRPO, A3C, and GAIL. We show that duality is not uncommon in reinforcement learning, especially when value iteration, or dynamic programming, is used or when ﬁrst or second order approximations are made to transform initially intractable problems into tractable convex programs.},
	language = {en},
	urldate = {2024-10-09},
	publisher = {arXiv},
	author = {Pasula, Pranay},
	month = jul,
	year = {2020},
	note = {arXiv:2007.09998 [cs, math, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control},
	file = {Pasula - 2020 - Lagrangian Duality in Reinforcement Learning.pdf:C\:\\Users\\14532\\Zotero\\storage\\ZCTQ6IRA\\Pasula - 2020 - Lagrangian Duality in Reinforcement Learning.pdf:application/pdf},
}

@misc{yang_policy_2023,
	title = {Policy {Representation} via {Diffusion} {Probability} {Model} for {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2305.13122},
	abstract = {Popular reinforcement learning (RL) algorithms tend to produce a unimodal policy distribution, which weakens the expressiveness of complicated policy and decays the ability of exploration. The diffusion probability model is powerful to learn complicated multimodal distributions, which has shown promising and potential applications to RL. In this paper, we formally build a theoretical foundation of policy representation via the diffusion probability model and provide practical implementations of diffusion policy for online model-free RL. Concretely, we character diffusion policy as a stochastic process, which is a new approach to representing a policy. Then we present a convergence guarantee for diffusion policy, which provides a theory to understand the multimodality of diffusion policy. Furthermore, we propose the DIPO which is an implementation for model-free online RL with DIffusion POlicy. To the best of our knowledge, DIPO is the first algorithm to solve model-free online RL problems with the diffusion model. Finally, extensive empirical results show the effectiveness and superiority of DIPO on the standard continuous control Mujoco benchmark.},
	urldate = {2024-10-19},
	publisher = {arXiv},
	author = {Yang, Long and Huang, Zhixiong and Lei, Fenghao and Zhong, Yucun and Yang, Yiming and Fang, Cong and Wen, Shiting and Zhou, Binbin and Lin, Zhouchen},
	month = may,
	year = {2023},
	note = {arXiv:2305.13122},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\14532\\Zotero\\storage\\U8ARJ24Z\\Yang 等 - 2023 - Policy Representation via Diffusion Probability Mo.pdf:application/pdf;Snapshot:C\:\\Users\\14532\\Zotero\\storage\\MBGQRBPD\\2305.html:text/html},
}

@misc{ajay_is_2023,
	title = {Is {Conditional} {Generative} {Modeling} all you need for {Decision}-{Making}?},
	url = {http://arxiv.org/abs/2211.15657},
	abstract = {Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone. We investigate whether these methods can directly address the problem of sequential decision-making. We view decision-making not through the lens of reinforcement learning (RL), but rather through conditional generative modeling. To our surprise, we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks. By modeling a policy as a return-conditional diffusion model, we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL. We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables: constraints and skills. Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills. Our results illustrate that conditional generative modeling is a powerful tool for decision-making.},
	urldate = {2024-10-19},
	publisher = {arXiv},
	author = {Ajay, Anurag and Du, Yilun and Gupta, Abhi and Tenenbaum, Joshua and Jaakkola, Tommi and Agrawal, Pulkit},
	month = jul,
	year = {2023},
	note = {arXiv:2211.15657},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\14532\\Zotero\\storage\\KLB6TKG5\\Ajay 等 - 2023 - Is Conditional Generative Modeling all you need fo.pdf:application/pdf;Snapshot:C\:\\Users\\14532\\Zotero\\storage\\UV7M6KB8\\2211.html:text/html},
}

@misc{song_score-based_2021,
	title = {Score-{Based} {Generative} {Modeling} through {Stochastic} {Differential} {Equations}},
	url = {http://arxiv.org/abs/2011.13456},
	abstract = {Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field ({\textbackslash}aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.},
	urldate = {2024-10-19},
	publisher = {arXiv},
	author = {Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
	month = feb,
	year = {2021},
	note = {arXiv:2011.13456},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\14532\\Zotero\\storage\\LW3BMKWB\\Song 等 - 2021 - Score-Based Generative Modeling through Stochastic.pdf:application/pdf;Snapshot:C\:\\Users\\14532\\Zotero\\storage\\CUXFDI7Q\\2011.html:text/html},
}

@misc{guo_gradient_2024,
	title = {Gradient {Guidance} for {Diffusion} {Models}: {An} {Optimization} {Perspective}},
	shorttitle = {Gradient {Guidance} for {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2404.14743},
	abstract = {Diffusion models have demonstrated empirical successes in various applications and can be adapted to task-specific needs via guidance. This paper studies a form of gradient guidance for adapting a pre-trained diffusion model towards optimizing user-specified objectives. We establish a mathematical framework for guided diffusion to systematically study its optimization theory and algorithmic design. Our theoretical analysis spots a strong link between guided diffusion models and optimization: gradient-guided diffusion models are essentially sampling solutions to a regularized optimization problem, where the regularization is imposed by the pre-training data. As for guidance design, directly bringing in the gradient of an external objective function as guidance would jeopardize the structure in generated samples. We investigate a modified form of gradient guidance based on a forward prediction loss, which leverages the information in pre-trained score functions and provably preserves the latent structure. We further consider an iteratively fine-tuned version of gradient-guided diffusion where guidance and score network are both updated with newly generated samples. This process mimics a first-order optimization iteration in expectation, for which we proved O˜(1/K) convergence rate to the global optimum when the objective function is concave. Our code will be released at https://github.com/yukang123/GGDMOptim.git.},
	language = {en},
	urldate = {2024-10-19},
	publisher = {arXiv},
	author = {Guo, Yingqing and Yuan, Hui and Yang, Yukang and Chen, Minshuo and Wang, Mengdi},
	month = oct,
	year = {2024},
	note = {arXiv:2404.14743 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Guo 等 - 2024 - Gradient Guidance for Diffusion Models An Optimiz.pdf:C\:\\Users\\14532\\Zotero\\storage\\PGRIC3DY\\Guo 等 - 2024 - Gradient Guidance for Diffusion Models An Optimiz.pdf:application/pdf},
}

@article{yuan_reward-directed_nodate,
	title = {Reward-{Directed} {Conditional} {Diffusion}: {Provable} {Distribution} {Estimation} and {Reward} {Improvement}},
	abstract = {We explore the methodology and theory of reward-directed generation via conditional diffusion models. Directed generation aims to generate samples with desired properties as measured by a reward function, which has broad applications in generative AI, reinforcement learning, and computational biology. We consider the common learning scenario where the dataset consists of majorly unlabeled data and a small set of data with noisy reward labels. Our approach leverages a learned reward function on the smaller data set as a pseudolabeler to label the unlabelled data. After pseudo-labelling, a conditional diffusion model (CDM) is trained on the data and samples are generated by setting a target value a as the condition in CDM. From a theoretical standpoint, we show that this directed generator can effectively learn and sample from the reward-conditioned data distribution: 1. our model is capable of recovering the data’s latent subspace representation. 2. the model generates samples moving closer to the user-specified target. The improvement in rewards of samples is influenced by a interplay between the strength of the reward signal, the distribution shift, and the cost of off-support extrapolation. We provide empirical results to validate our theory and highlight the relationship between the strength of extrapolation and the quality of generated samples. Our code is available at https://github.com/Kaffaljidhmah2/RCGDM.},
	language = {en},
	author = {Yuan, Hui and Huang, Kaixuan},
	file = {Yuan 和 Huang - Reward-Directed Conditional Diffusion Provable Di.pdf:C\:\\Users\\14532\\Zotero\\storage\\LXY9ZZRN\\Yuan 和 Huang - Reward-Directed Conditional Diffusion Provable Di.pdf:application/pdf},
}

@misc{xie_ode-based_2024,
	title = {{ODE}-based {Learning} to {Optimize}},
	url = {http://arxiv.org/abs/2406.02006},
	abstract = {Recent years have seen a growing interest in understanding acceleration methods through the lens of ordinary differential equations (ODEs). Despite the theoretical advancements, translating the rapid convergence observed in continuous-time models to discrete-time iterative methods poses significant challenges. In this paper, we present a comprehensive framework integrating the inertial systems with Hessian-driven damping equation (ISHD) and learning-based approaches for developing optimization methods through a deep synergy of theoretical insights. We first establish the convergence condition for ensuring the convergence of the solution trajectory of ISHD. Then, we show that provided the stability condition, another relaxed requirement on the coefficients of ISHD, the sequence generated through the explicit Euler discretization of ISHD converges, which gives a large family of practical optimization methods. In order to select the best optimization method in this family for certain problems, we introduce the stopping time, the time required for an optimization method derived from ISHD to achieve a predefined level of suboptimality. Then, we formulate a novel learning to optimize (L2O) problem aimed at minimizing the stopping time subject to the convergence and stability condition. To navigate this learning problem, we present an algorithm combining stochastic optimization and the penalty method (StoPM). The convergence of StoPM using the conservative gradient is proved. Empirical validation of our framework is conducted through extensive numerical experiments across a diverse set of optimization problems. These experiments showcase the superior performance of the learned optimization methods.},
	urldate = {2024-10-27},
	publisher = {arXiv},
	author = {Xie, Zhonglin and Yin, Wotao and Wen, Zaiwen},
	month = jun,
	year = {2024},
	note = {arXiv:2406.02006},
	keywords = {Computer Science - Artificial Intelligence, Mathematics - Optimization and Control},
	file = {Full Text PDF:C\:\\Users\\14532\\Zotero\\storage\\V2ZRK2E8\\Xie 等 - 2024 - ODE-based Learning to Optimize.pdf:application/pdf;Snapshot:C\:\\Users\\14532\\Zotero\\storage\\QVJUNASQ\\2406.html:text/html},
}

@misc{chen_learning_2024-2,
	title = {Learning to optimize: {A} tutorial for continuous and mixed-integer optimization},
	shorttitle = {Learning to optimize},
	url = {http://arxiv.org/abs/2405.15251},
	abstract = {Learning to Optimize (L2O) stands at the intersection of traditional optimization and machine learning, utilizing the capabilities of machine learning to enhance conventional optimization techniques. As real-world optimization problems frequently share common structures, L2O provides a tool to exploit these structures for better or faster solutions. This tutorial dives deep into L2O techniques, introducing how to accelerate optimization algorithms, promptly estimate the solutions, or even reshape the optimization problem itself, making it more adaptive to real-world applications. By considering the prerequisites for successful applications of L2O and the structure of the optimization problems at hand, this tutorial provides a comprehensive guide for practitioners and researchers alike.},
	urldate = {2024-10-27},
	publisher = {arXiv},
	author = {Chen, Xiaohan and Liu, Jialin and Yin, Wotao},
	month = may,
	year = {2024},
	note = {arXiv:2405.15251},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control},
	file = {Full Text PDF:C\:\\Users\\14532\\Zotero\\storage\\3LR9F5KA\\Chen 等 - 2024 - Learning to optimize A tutorial for continuous an.pdf:application/pdf;Snapshot:C\:\\Users\\14532\\Zotero\\storage\\7WSKSFW7\\2405.html:text/html},
}

@misc{zhang_solving_2024,
	title = {Solving {General} {Natural}-{Language}-{Description} {Optimization} {Problems} with {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2407.07924},
	abstract = {Optimization problems seek to find the best solution to an objective under a set of constraints, and have been widely investigated in real-world applications. Modeling and solving optimization problems in a specific domain typically require a combination of domain knowledge, mathematical skills, and programming ability, making it difficult for general users and even domain professionals. In this paper, we propose a novel framework called OptLLM that augments LLMs with external solvers. Specifically, OptLLM accepts user queries in natural language, convert them into mathematical formulations and programming codes, and calls the solvers to calculate the results for decision-making. In addition, OptLLM supports multi-round dialogues to gradually refine the modeling and solving of optimization problems. To illustrate the effectiveness of OptLLM, we provide tutorials on three typical optimization applications and conduct experiments on both prompt-based GPT models and a fine-tuned Qwen model using a large-scale selfdeveloped optimization dataset. Experimental results show that OptLLM works with various LLMs, and the fine-tuned model achieves an accuracy boost compared to the promptbased models. Some features of OptLLM framework have been available for trial since June 2023 (https://opt.alibabacloud.com/chat or https://opt.aliyun.com/chat).},
	urldate = {2024-10-27},
	publisher = {arXiv},
	author = {Zhang, Jihai and Wang, Wei and Guo, Siyan and Wang, Li and Lin, Fangquan and Yang, Cheng and Yin, Wotao},
	month = jul,
	year = {2024},
	note = {arXiv:2407.07924},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Optimization and Control, Computer Science - Computation and Language},
	file = {Full Text PDF:C\:\\Users\\14532\\Zotero\\storage\\WP93TBAQ\\Zhang 等 - 2024 - Solving General Natural-Language-Description Optim.pdf:application/pdf;Snapshot:C\:\\Users\\14532\\Zotero\\storage\\5TKUX229\\2407.html:text/html},
}

@misc{huang_restarted_2024,
	title = {Restarted {Primal}-{Dual} {Hybrid} {Conjugate} {Gradient} {Method} for {Large}-{Scale} {Quadratic} {Programming}},
	copyright = {All rights reserved},
	url = {http://arxiv.org/abs/2405.16160},
	abstract = {Convex quadratic programming (QP) is an essential class of optimization problems with broad applications across various fields. Traditional QP solvers, typically based on simplex or barrier methods, face significant scalability challenges. In response to these limitations, recent research has shifted towards matrix-free first-order methods to enhance scalability in QP. Among these, the restarted accelerated primal-dual hybrid gradient (rAPDHG) method, proposed by Lu, has gained notable attention due to its linear convergence rate to an optimal solution and its straightforward implementation on Graphics Processing Units (GPUs). Building on this framework, this paper introduces a restarted primal-dual hybrid conjugate gradient (PDHCG) method, which incorporates conjugate gradient (CG) techniques to address the primal subproblems inexactly. We demonstrate that PDHCG maintains a linear convergence rate with an improved convergence constant and is also straightforward to implement on GPUs. Extensive numerical experiments on both synthetic and real-world datasets demonstrate that our method significantly reduces the number of iterations required to achieve the desired accuracy compared to rAPDHG. Additionally, the GPU implementation of our method achieves state-of-the-art performance on large-scale problems. In most large-scale scenarios, our method is approximately 5 times faster than rAPDHG and about 100 times faster than other existing methods. These results highlight the substantial potential of the proposed PDHCG method to greatly improve both the efficiency and scalability of solving complex quadratic programming challenges.},
	urldate = {2024-11-04},
	publisher = {arXiv},
	author = {Huang, Yicheng and Zhang, Wanyu and Li, Hongpei and Ge, Dongdong and Liu, Huikang and Ye, Yinyu},
	month = oct,
	year = {2024},
	note = {arXiv:2405.16160},
	keywords = {Mathematics - Optimization and Control},
	file = {Full Text PDF:C\:\\Users\\14532\\Zotero\\storage\\UCZ3KHP6\\Huang 等 - 2024 - Restarted Primal-Dual Hybrid Conjugate Gradient Method for Large-Scale Quadratic Programming.pdf:application/pdf;Snapshot:C\:\\Users\\14532\\Zotero\\storage\\356JJK3W\\2405.html:text/html},
}

@misc{li_solving_2024,
	title = {Solving {Integrated} {Process} {Planning} and {Scheduling} {Problem} via {Graph} {Neural} {Network} {Based} {Deep} {Reinforcement} {Learning}},
	copyright = {All rights reserved},
	url = {http://arxiv.org/abs/2409.00968},
	doi = {10.48550/arXiv.2409.00968},
	abstract = {The Integrated Process Planning and Scheduling (IPPS) problem combines process route planning and shop scheduling to achieve high efficiency in manufacturing and maximize resource utilization, which is crucial for modern manufacturing systems. Traditional methods using Mixed Integer Linear Programming (MILP) and heuristic algorithms can not well balance solution quality and speed when solving IPPS. In this paper, we propose a novel end-to-end Deep Reinforcement Learning (DRL) method. We model the IPPS problem as a Markov Decision Process (MDP) and employ a Heterogeneous Graph Neural Network (GNN) to capture the complex relationships among operations, machines, and jobs. To optimize the scheduling strategy, we use Proximal Policy Optimization (PPO). Experimental results show that, compared to traditional methods, our approach significantly improves solution efficiency and quality in large-scale IPPS instances, providing superior scheduling strategies for modern intelligent manufacturing systems.},
	urldate = {2024-11-04},
	publisher = {arXiv},
	author = {Li, Hongpei and Zhang, Han and He, Ziyan and Jia, Yunkai and Jiang, Bo and Huang, Xiang and Ge, Dongdong},
	month = sep,
	year = {2024},
	note = {arXiv:2409.00968},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Optimization and Control},
	file = {Preprint PDF:C\:\\Users\\14532\\Zotero\\storage\\2IIZVLFA\\Li 等 - 2024 - Solving Integrated Process Planning and Scheduling Problem via Graph Neural Network Based Deep Reinf.pdf:application/pdf;Snapshot:C\:\\Users\\14532\\Zotero\\storage\\4MYBDZZ6\\2409.html:text/html},
}
